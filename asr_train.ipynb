{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "olive-saturday",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import copy\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from common import helpers\n",
    "\n",
    "from common.dataset import AudioDataset, get_data_loader\n",
    "from common.features import BaseFeatures, FilterbankFeatures\n",
    "from common.helpers import (Checkpointer, greedy_wer, num_weights, print_once,\n",
    "                            process_evaluation_epoch)\n",
    "##from common.tb_dllogger import flush_log, init_log, log\n",
    "from transducer import config\n",
    "from transducer.model import Transducer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "charming-broadway",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args():\n",
    "    \n",
    "    model_str = \"./configs/transducer_asr.yaml\"\n",
    "    dataset_str = \"../LibriSpeech/\"\n",
    "    traindata_str = [dataset_str+\"librispeech-train-clean-100-wav.json\"]\n",
    "    valdata_str = [dataset_str+\"librispeech-dev-clean-wav.json\"]\n",
    "    out_str = \"./results/\"\n",
    "   \n",
    "    \n",
    "    parser = argparse.ArgumentParser(description='Transducer')\n",
    "\n",
    "    training = parser.add_argument_group('training setup')\n",
    "    training.add_argument('--epochs', default=10, type=int,\n",
    "                          help='Number of epochs for the entire training; influences the lr schedule')\n",
    "   \n",
    "    training.add_argument('--seed', default=42, type=int, help='Random seed')\n",
    "   \n",
    "    optim = parser.add_argument_group('optimization setup')\n",
    "    optim.add_argument('--batch_size', default=4, type=int,\n",
    "                       help='Global batch size')\n",
    "    optim.add_argument('--lr', default=1e-4, type=float,\n",
    "                       help='Peak learning rate')\n",
    "    optim.add_argument(\"--lr_exp_gamma\", default=0.99, type=float,\n",
    "                       help='gamma factor for exponential lr scheduler')\n",
    "   \n",
    "    io = parser.add_argument_group('feature and checkpointing setup')\n",
    "   \n",
    "    io.add_argument('--model_config', type=str, default = model_str,\n",
    "                    help='Path of the model configuration file')\n",
    "    io.add_argument('--train_manifests', type=str, default=traindata_str, nargs='+',\n",
    "                    help='Paths of the training dataset manifest file')\n",
    "    io.add_argument('--val_manifests', type=str, default=valdata_str, nargs='+',\n",
    "                    help='Paths of the evaluation datasets manifest files')\n",
    "    io.add_argument('--max_duration', type=float,\n",
    "                    help='Discard samples longer than max_duration')\n",
    "    io.add_argument('--pad_to_max_duration', action='store_true', default=False,\n",
    "                    help='Pad training sequences to max_duration')\n",
    "    io.add_argument('--dataset_dir', default=dataset_str, type=str,\n",
    "                    help='Root dir of dataset')\n",
    "    io.add_argument('--output_dir', type=str, default=out_str,\n",
    "                    help='Directory for logs and checkpoints')\n",
    "    io.add_argument('--log_file', type=str, default=None,\n",
    "                    help='Path to save the training logfile.')\n",
    "    return parser.parse_args(\"\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "built-production",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user1/.conda/envs/w266_project/lib/python3.8/site-packages/torch/functional.py:580: UserWarning: stft will soon require the return_complex parameter be given for real inputs, and will further require that return_complex=True in a future PyTorch release. (Triggered internally at  /tmp/pip-req-build-c9mgsz4q/aten/src/ATen/native/SpectralOps.cpp:639.)\n",
      "  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore\n"
     ]
    }
   ],
   "source": [
    "def data_generator(args, config, symbols):\n",
    "    \n",
    "    print('Setting up datasets...')\n",
    "    cfg = config.load(args.model_config)\n",
    "    config.apply_duration_flags(cfg, args.max_duration, args.pad_to_max_duration)\n",
    "    \n",
    "    train_dataset_kw, train_features_kw = config.input(cfg, 'train')\n",
    "    train_dataset = AudioDataset(args.dataset_dir,\n",
    "                                 args.train_manifests,\n",
    "                                 symbols,\n",
    "                                 **train_dataset_kw)\n",
    "    \n",
    "    train_loader = get_data_loader(train_dataset,\n",
    "                                       args.batch_size,\n",
    "                                       multi_gpu=0,\n",
    "                                       shuffle=True,\n",
    "                                       num_workers=0)\n",
    "    \n",
    "    train_feat_proc = FilterbankFeatures(**train_features_kw)\n",
    "    \n",
    "    val_dataset_kw, val_features_kw = config.input(cfg, 'val')\n",
    "    val_dataset = AudioDataset(args.dataset_dir,\n",
    "                                   args.val_manifests,\n",
    "                                   symbols,\n",
    "                                   **val_dataset_kw)\n",
    "    val_loader = get_data_loader(val_dataset,\n",
    "                                     args.batch_size,\n",
    "                                     multi_gpu=0,\n",
    "                                     shuffle=False,\n",
    "                                     num_workers=0,\n",
    "                                     drop_last=False)\n",
    "    \n",
    "    val_feat_proc = FilterbankFeatures(**val_features_kw)\n",
    "    \n",
    "    return train_loader, train_feat_proc, val_loader, val_feat_proc\n",
    "    \n",
    "def main():\n",
    "    args = parse_args()\n",
    "    multi_gpu = 0\n",
    "    args.amp = False\n",
    "    torch.manual_seed(args.seed + 1)\n",
    "    np.random.seed(args.seed + 2)\n",
    "    random.seed(args.seed + 3)\n",
    "    cfg = config.load(args.model_config)\n",
    "\n",
    "    symbols = cfg['labels'] + ['<BLANK>']\n",
    "    \n",
    "    \n",
    "    train_loader, train_feat_proc, val_loader, val_feat_proc = data_generator(args, config, symbols)\n",
    "\n",
    "    \n",
    "    #Configure model and optimizer\n",
    "    \n",
    "    num_inputs = 64 #train_features_kw['n_filt']\n",
    "    model = Transducer(num_inputs, 32)\n",
    "    lr = args.lr\n",
    "    lr_gamma = args.lr_exp_gamma\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=lr_gamma)\n",
    "    batch_size = args.batch_size    \n",
    "        \n",
    "    num_epochs = args.epochs\n",
    "    while(num_epochs>0):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        num_samples = 0\n",
    "        test_loss = 0\n",
    "        for idx, batch in enumerate(train_loader):\n",
    "            audio, audio_lens, txt, txt_lens = batch\n",
    "            feat, feat_lens = train_feat_proc(audio, audio_lens, args.amp)\n",
    "            #feat, feat_lens = audio, audio_lens\n",
    "            feat = feat.transpose(1, 2)\n",
    "            feat = feat.to(model.device)\n",
    "            txt = txt.to(model.device)\n",
    "            batch_size = feat.shape[0]\n",
    "            loss = model.compute_loss(feat,txt,feat_lens,txt_lens)\n",
    "            num_samples += batch_size\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * batch_size\n",
    "        train_loss /= num_samples\n",
    "     \n",
    "        lr_scheduler.step()  \n",
    "        num_samples = 0\n",
    "        model.eval()\n",
    "        for batch in val_loader:\n",
    "            audio, audio_lens, txt, txt_lens = batch\n",
    "            feat, feat_lens = val_feat_proc(audio, audio_lens, args.amp)\n",
    "            feat = feat.transpose(1, 2)\n",
    "            \n",
    "            feat = feat.to(model.device)\n",
    "            txt = txt.to(model.device)\n",
    "            batch_size = feat.shape[0]\n",
    "            loss = model.compute_loss(feat,txt,feat_lens,txt_lens)\n",
    "            \n",
    "            num_samples += batch_size\n",
    "            test_loss += loss.item() * batch_size\n",
    "            \n",
    "            \n",
    "        test_loss /= num_samples\n",
    "        \n",
    "        print(train_loss, test_loss)    \n",
    "        num_epochs -= 1    \n",
    "     \n",
    "main()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bridal-childhood",
   "metadata": {},
   "outputs": [],
   "source": [
    "joiner_dim = 512\n",
    "encoder_dim = 512\n",
    "predictor_dim = 512\n",
    "\n",
    "\n",
    "\n",
    "class Encoder(torch.nn.Module):\n",
    "  def __init__(self, num_inputs):\n",
    "    super(Encoder, self).__init__()\n",
    "    \n",
    "    #self.embed = torch.nn.Embedding(num_inputs, encoder_dim)\n",
    "    self.rnn = torch.nn.GRU(input_size=num_inputs, hidden_size=encoder_dim, num_layers=3, batch_first=True, bidirectional=True, dropout=0.1)\n",
    "    self.linear = torch.nn.Linear(encoder_dim*2, joiner_dim)\n",
    "\n",
    "  def forward(self, x):\n",
    "    out = self.rnn(x)[0]\n",
    "    out = self.linear(out)\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "class Predictor(torch.nn.Module):\n",
    "  def __init__(self, num_outputs):\n",
    "    super(Predictor, self).__init__()\n",
    "    self.blank_index = num_outputs-1\n",
    "    self.embed = torch.nn.Embedding(num_outputs, 32)\n",
    "    self.rnn = torch.nn.GRUCell(input_size=32, hidden_size=predictor_dim)\n",
    "    self.linear = torch.nn.Linear(predictor_dim, joiner_dim)\n",
    "    \n",
    "    self.initial_state = torch.nn.Parameter(torch.randn(predictor_dim))\n",
    "    self.start_symbol = self.blank_index # In the original paper, a vector of 0s is used; just using the null index instead is easier when using an Embedding layer.\n",
    "\n",
    "  def forward_one_step(self, input, previous_state):\n",
    "    embedding = self.embed(input)\n",
    "    state = self.rnn.forward(embedding, previous_state)\n",
    "    out = self.linear(state)\n",
    "    return out, state\n",
    "\n",
    "  def forward(self, y):\n",
    "    batch_size = y.shape[0]\n",
    "    U = y.shape[1]\n",
    "    outs = []\n",
    "    state = torch.stack([self.initial_state] * batch_size).to(y.device)\n",
    "    for u in range(U+1): # need U+1 to get null output for final timestep \n",
    "      if u == 0:\n",
    "        decoder_input = torch.tensor([self.start_symbol] * batch_size).to(y.device)\n",
    "      else:\n",
    "        decoder_input = y[:,u-1]\n",
    "      out, state = self.forward_one_step(decoder_input, state)\n",
    "      outs.append(out)\n",
    "    out = torch.stack(outs, dim=1)\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "class Joiner(torch.nn.Module):\n",
    "  def __init__(self, num_outputs):\n",
    "    super(Joiner, self).__init__()\n",
    "    self.linear = torch.nn.Linear(joiner_dim, num_outputs)\n",
    "\n",
    "  def forward(self, encoder_out, predictor_out):\n",
    "    out = encoder_out + predictor_out\n",
    "    out = torch.nn.functional.relu(out)\n",
    "    out = self.linear(out)\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "class Transducer(torch.nn.Module):\n",
    "  def __init__(self, num_inputs, num_outputs):\n",
    "    super(Transducer, self).__init__()\n",
    "    self.blank_index = num_outputs-1\n",
    "    self.encoder = Encoder(num_inputs)\n",
    "    self.predictor = Predictor(num_outputs)\n",
    "    self.joiner = Joiner(num_outputs)\n",
    "    \n",
    "\n",
    "    if torch.cuda.is_available(): self.device = \"cuda\"\n",
    "    else: self.device = \"cpu\"\n",
    "    self.to(self.device)\n",
    "\n",
    "  def compute_forward_prob(self, joiner_out, T, U, y):\n",
    "    \"\"\"\n",
    "    joiner_out: tensor of shape (B, T_max, U_max+1, #labels)\n",
    "    T: list of input lengths\n",
    "    U: list of output lengths \n",
    "    y: label tensor (B, U_max+1)\n",
    "    \"\"\"\n",
    "    B = joiner_out.shape[0]\n",
    "    T_max = joiner_out.shape[1]\n",
    "    U_max = joiner_out.shape[2] - 1\n",
    "    log_alpha = torch.zeros(B, T_max, U_max+1).to(y.device)\n",
    "    for t in range(T_max):\n",
    "      for u in range(U_max+1):\n",
    "          if u == 0:\n",
    "            if t == 0:\n",
    "              log_alpha[:, t, u] = 0.\n",
    "\n",
    "            else: #t > 0\n",
    "              log_alpha[:, t, u] = log_alpha[:, t-1, u] + joiner_out[:, t-1, 0, self.blank_index] \n",
    "                  \n",
    "          else: #u > 0\n",
    "            if t == 0:\n",
    "              log_alpha[:, t, u] = log_alpha[:, t,u-1] + torch.gather(joiner_out[:, t, u-1], dim=1, index=y[:,u-1].view(-1,1) ).reshape(-1)\n",
    "            \n",
    "            else: #t > 0\n",
    "              log_alpha[:, t, u] = torch.logsumexp(torch.stack([\n",
    "                  log_alpha[:, t-1, u] + joiner_out[:, t-1, u, self.blank_index],\n",
    "                  log_alpha[:, t, u-1] + torch.gather(joiner_out[:, t, u-1], dim=1, index=y[:,u-1].view(-1,1) ).reshape(-1)\n",
    "              ]), dim=0)\n",
    "    \n",
    "    log_probs = []\n",
    "    for b in range(B):\n",
    "      log_prob = log_alpha[b, T[b]-1, U[b]] + joiner_out[b, T[b]-1, U[b], self.blank_index]\n",
    "      log_probs.append(log_prob)\n",
    "    log_probs = torch.stack(log_probs) \n",
    "    return log_prob\n",
    "\n",
    "  def compute_loss(self, x, y, T, U):\n",
    "    encoder_out = self.encoder.forward(x)\n",
    "    predictor_out = self.predictor.forward(y)\n",
    "    joiner_out = self.joiner.forward(encoder_out.unsqueeze(2), predictor_out.unsqueeze(1)).log_softmax(3)\n",
    "    loss = -self.compute_forward_prob(joiner_out, T, U, y).mean()\n",
    "    return loss\n",
    "    \n",
    "  def greedy_search(self, x, T):\n",
    "     y_batch = []\n",
    "     B = len(x)\n",
    "     print(B, len(T))  \n",
    "\n",
    "     encoder_out = self.encoder.forward(x)\n",
    "     U_max = 200\n",
    "     for b in range(B):\n",
    "        t = 0; u = 0; y = [self.predictor.start_symbol]; predictor_state = self.predictor.initial_state.unsqueeze(0)\n",
    "        while t < T[b] and u < U_max:\n",
    "           predictor_input = torch.tensor([ y[-1] ]).to(x.device)\n",
    "           g_u, predictor_state = self.predictor.forward_one_step(predictor_input, predictor_state)\n",
    "           f_t = encoder_out[b, t]\n",
    "           h_t_u = self.joiner.forward(f_t, g_u)\n",
    "           argmax = h_t_u.max(-1)[1].item()\n",
    "           if argmax == self.blank_index:\n",
    "              t += 1\n",
    "           else: # argmax == a label\n",
    "              u += 1\n",
    "              y.append(argmax)\n",
    "        y_batch.append(y[1:]) # remove start symbol\n",
    "     return y_batch   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outer-shower",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "w266_project",
   "language": "python",
   "name": "w266_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

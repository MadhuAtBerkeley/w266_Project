{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BbwgR5UdNkkm"
   },
   "source": [
    "# Base Model Seq2Seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q-iHU02C7fAj",
    "outputId": "118cc9b3-228a-4aee-cd32-3692a636d570"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/home/user1/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "Requirement already satisfied: unidecode in /home/user1/.conda/envs/w266_env/lib/python3.8/site-packages (1.2.0)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import string\n",
    "!pip install unidecode\n",
    "import unidecode\n",
    "import numpy as np\n",
    "import itertools\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CTfRgwxmjv1B"
   },
   "source": [
    "# Building blocks\n",
    "\n",
    "First, we will define the encode and decoder\n",
    "\n",
    "<img src=\"\" width=\"25%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "B7mLFyUG7kJH"
   },
   "outputs": [],
   "source": [
    "NULL_INDEX = 0\n",
    "\n",
    "encoder_dim = 1024\n",
    "predictor_dim = 1024\n",
    "joiner_dim = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "KE7j2T5EY33-"
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        \n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        \n",
    "        #src = [src len, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        \n",
    "        #embedded = [src len, batch size, emb dim]\n",
    "        \n",
    "        outputs, (hidden, cell) = self.rnn(embedded)\n",
    "        \n",
    "        #outputs = [src len, batch size, hid dim * n directions]\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        #cell = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        #outputs are always from the top hidden layer\n",
    "        \n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "Vlzca1orZDLa"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.output_dim = output_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        \n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n",
    "        \n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden, cell):\n",
    "        \n",
    "        #input = [batch size]\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        #cell = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        #n directions in the decoder will both always be 1, therefore:\n",
    "        #hidden = [n layers, batch size, hid dim]\n",
    "        #context = [n layers, batch size, hid dim]\n",
    "        \n",
    "        input = input.unsqueeze(0)\n",
    "        \n",
    "        #input = [1, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        \n",
    "        #embedded = [1, batch size, emb dim]\n",
    "                \n",
    "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
    "        \n",
    "        #output = [seq len, batch size, hid dim * n directions]\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        #cell = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        #seq len and n directions will always be 1 in the decoder, therefore:\n",
    "        #output = [1, batch size, hid dim]\n",
    "        #hidden = [n layers, batch size, hid dim]\n",
    "        #cell = [n layers, batch size, hid dim]\n",
    "        \n",
    "        prediction = self.fc_out(output.squeeze(0))\n",
    "        \n",
    "        #prediction = [batch size, output dim]\n",
    "        \n",
    "        return prediction, hidden, cell\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "sYSagKi-gHM4"
   },
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "        assert encoder.hid_dim == decoder.hid_dim, \\\n",
    "            \"Hidden dimensions of encoder and decoder must be equal!\"\n",
    "        assert encoder.n_layers == decoder.n_layers, \\\n",
    "            \"Encoder and decoder must have equal number of layers!\"\n",
    "        \n",
    "    def forward(self, src, trg, teacher_forcing_ratio = 0.95):\n",
    "        \n",
    "        #src = [src len, batch size]\n",
    "        #trg = [trg len, batch size]\n",
    "        #teacher_forcing_ratio is probability to use teacher forcing\n",
    "        #e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time\n",
    "        \n",
    "        batch_size = trg.shape[1]\n",
    "        trg_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        \n",
    "        #tensor to store decoder outputs\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        \n",
    "        #last hidden state of the encoder is used as the initial hidden state of the decoder\n",
    "        hidden, cell = self.encoder(src)\n",
    "        \n",
    "        #first input to the decoder is the <sos>\n",
    "        input = trg[0,:]\n",
    "        \n",
    "        for t in range(1, trg_len):\n",
    "            \n",
    "            #insert input token embedding, previous hidden and previous cell states\n",
    "            #receive output tensor (predictions) and new hidden and cell states\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "            \n",
    "            #place predictions in a tensor holding predictions for each token\n",
    "            outputs[t] = output\n",
    "            \n",
    "            #decide if we are going to use teacher forcing or not\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            \n",
    "            #get the highest predicted token from our predictions\n",
    "            top1 = output.argmax(1) \n",
    "            \n",
    "            #if teacher forcing, use actual next token as next input\n",
    "            #if not, use predicted token\n",
    "            input = trg[t] if teacher_force else top1\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "RWtkoXH6U8Pm"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "INPUT_DIM = 32\n",
    "OUTPUT_DIM = 32\n",
    "ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "HID_DIM = 1024\n",
    "N_LAYERS = 2\n",
    "ENC_DROPOUT = 0.2\n",
    "DEC_DROPOUT = 0.2\n",
    "START_INDEX = 0\n",
    "BLANK_INDEX = 27\n",
    "PAD_INDEX = 30\n",
    "END_INDEX = 31\n",
    "\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
    "\n",
    "model = Seq2Seq(enc, dec, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "V0xeyb7Jf18_"
   },
   "outputs": [],
   "source": [
    "def greedy_search(self, x, T):\n",
    "  y_batch = []\n",
    "  B = len(x)\n",
    "  print(B, len(T))  \n",
    "\n",
    "  encoder_out = self.encoder.forward(x)\n",
    "  U_max = 200\n",
    "  for b in range(B):\n",
    "    t = 0; u = 0; y = [self.predictor.start_symbol]; predictor_state = self.predictor.initial_state.unsqueeze(0)\n",
    "    while t < T[b] and u < U_max:\n",
    "      predictor_input = torch.tensor([ y[-1] ]).to(x.device)\n",
    "      g_u, predictor_state = self.predictor.forward_one_step(predictor_input, predictor_state)\n",
    "      f_t = encoder_out[b, t]\n",
    "      h_t_u = self.joiner.forward(f_t, g_u)\n",
    "      argmax = h_t_u.max(-1)[1].item()\n",
    "      if argmax == NULL_INDEX:\n",
    "        t += 1\n",
    "      else: # argmax == a label\n",
    "        u += 1\n",
    "        y.append(argmax)\n",
    "    y_batch.append(y[1:]) # remove start symbol\n",
    "  return y_batch\n",
    "\n",
    "#Transducer.greedy_search = greedy_search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ff9raB0jVGzN"
   },
   "source": [
    "# Some utilities\n",
    "\n",
    "Here we will add a bit of boilerplate code for training and loading data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5b17OQm4WdVy",
    "outputId": "0a7d7b17-0b23-4ee8-e690-e70564917f04"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('weell pprinnccee soo ggeenoaa annd lucccca aarree nnow jjuusst fammillyy esstaatteess of tthe',\n",
       " 'well prince so genoa and lucca are now just family estates of the')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "char_labels = ['<start>', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's',\n",
    "              't', 'u', 'v', 'w', 'x', 'y', 'z', '<blank>', ' ', \"'\", '<pad>','<end>']\n",
    "char_dict = {'<start>':0, 'a':1, 'b':2, 'c':3, 'd':4, 'e':5, 'f':6, 'g':7, 'h':8, 'i':9, 'j':10, 'k':11, 'l':12, 'm':13, \n",
    "             'n':14, 'o':15, 'p':16, 'q':17, 'r':18, 's':19, 't':20, 'u':21, 'v':22, 'w':23, 'x':24, 'y':25, 'z':26, \n",
    "             '<blank>':27, ' ':28, \"'\":29, '<pad>':30,'<end>':31}\n",
    "char_inv_dict = {0:'', 1:'a', 2:'b', 3:'c', 4:'d', 5:'e', 6:'f', 7:'g', 8:'h', 9:'i', 10:'j', 11:'k', 12:'l',\n",
    "                 13:'m', 14:'n', 15:'o', 16:'p', 17:'q', 18:'r', 19:'s', 20:'t', 21:'u', 22:'v', 23:'w', 24:'x', 25:'y',\n",
    "                 26:'z', 27:'', 28:' ', 29:\"'\", 30:'', 31:''}\n",
    "\n",
    "class TextDataset(torch.utils.data.Dataset):\n",
    "  def __init__(self, lines, batch_size):\n",
    "    lines = list(filter((\"\\n\").__ne__, lines))\n",
    "\n",
    "    self.lines = lines # list of strings\n",
    "    collate = Collate()\n",
    "    self.loader = torch.utils.data.DataLoader(self, batch_size=batch_size, num_workers=0, shuffle=True, collate_fn=collate)\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.lines)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    line = self.lines[idx].replace(\"\\n\", \"\")\n",
    "    line =line.lower()\n",
    "    line = \"\".join(c for c in line if c in char_labels)\n",
    "    line = unidecode.unidecode(line) # remove special characters\n",
    "    x = \"\".join(c*random.randint(1,2) for c in line) \n",
    "    x = ' '.join(x.split())\n",
    "    y = line\n",
    "    return (x,y)\n",
    "\n",
    "def encode_string(s):\n",
    "\n",
    "  return [char_dict[c] for c in s]\n",
    "\n",
    "def decode_labels(l):\n",
    "  return \"\".join([char_inv_dict[c] for c in l])\n",
    "\n",
    "class Collate:\n",
    "  def __call__(self, batch):\n",
    "    \"\"\"\n",
    "    batch: list of tuples (input string, output string)\n",
    "    Returns a minibatch of strings, encoded as labels and padded to have the same length.\n",
    "    \"\"\"\n",
    "    x = []; y = []\n",
    "    batch_size = len(batch)\n",
    "    for index in range(batch_size):\n",
    "      x_,y_ = batch[index]\n",
    "      x.append([0]+encode_string(x_))\n",
    "      y.append([0]+encode_string(y_))\n",
    "\n",
    "    # pad all sequences to have same length\n",
    "    T = [len(x_) for x_ in x]\n",
    "    U = [len(y_) for y_ in y]\n",
    " \n",
    "    T_max = max(T)\n",
    "    U_max = max(U)\n",
    "    for index in range(batch_size):\n",
    "      x[index] += [PAD_INDEX] * (T_max - len(x[index])) +[31]\n",
    "      x[index] = torch.tensor(x[index])\n",
    "      y[index] += [PAD_INDEX] * (U_max - len(y[index]))+[31]\n",
    "      y[index] = torch.tensor(y[index])\n",
    "\n",
    "    # stack into single tensor\n",
    "    x = torch.stack(x)\n",
    "    y = torch.stack(y)\n",
    "    #T = torch.tensor(T)\n",
    "    #U = torch.tensor(U)\n",
    "\n",
    "    return (x,y) #,T,U)\n",
    "\n",
    "filename=\"../LibriSpeech/train-clean-100/transcripts_460.tsv\"\n",
    "with open(filename, \"r\") as f:\n",
    "   t_lines = f.readlines()\n",
    "t_lines = t_lines[1:]\n",
    "t_lines = [line.split('\\t')[2] for line in t_lines ]\n",
    "lines = []\n",
    "\n",
    "for line in t_lines:\n",
    "    temp = line.split(' ')\n",
    "    count = len(temp)//10\n",
    "    resid = len(temp)%10\n",
    "    \n",
    "    k = 0\n",
    "    while(count > 0):    \n",
    "        short_line = ' '.join(temp[k:k+10]) \n",
    "        lines.append(short_line)\n",
    "        count = count - 1\n",
    "        k = k + 10\n",
    "    if(resid > 0):\n",
    "        lines.append(' '.join(temp[k:k+resid]))   \n",
    "\n",
    "end = round(0.9 * len(lines))\n",
    "train_lines = lines[:end]\n",
    "test_lines = lines[end:]\n",
    "train_set = TextDataset(train_lines, batch_size=32)\n",
    "test_set = TextDataset(test_lines, batch_size=8)\n",
    "train_set.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55814 6201\n"
     ]
    }
   ],
   "source": [
    "print(len(train_lines), len(test_lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(32, 256)\n",
       "    (rnn): LSTM(256, 1024, num_layers=2, dropout=0.2)\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(32, 256)\n",
       "    (rnn): LSTM(256, 1024, num_layers=2, dropout=0.2)\n",
       "    (fc_out): Linear(in_features=1024, out_features=32, bias=True)\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "        \n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 27,344,928 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = PAD_INDEX)\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, batch in enumerate(iterator):\n",
    "        \n",
    "        src, trg = batch\n",
    "        trg = trg.transpose(1,0)\n",
    "        src = src.transpose(1,0)\n",
    "        optimizer.zero_grad()\n",
    "        src = src.to(device)\n",
    "        trg = trg.to(device)\n",
    "        output = model(src, trg)\n",
    "        \n",
    "        #trg = [trg len, batch size]\n",
    "        #output = [trg len, batch size, output dim]\n",
    "        \n",
    "        output_dim = output.shape[-1]\n",
    "        \n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        trg = trg[1:].reshape(-1)\n",
    "        \n",
    "        #trg = [(trg len - 1) * batch size]\n",
    "        #output = [(trg len - 1) * batch size, output dim]\n",
    "        \n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, batch in enumerate(iterator):\n",
    "\n",
    "            src, trg = batch\n",
    "            trg = trg.transpose(1,0)\n",
    "            src = src.transpose(1,0)\n",
    "            src = src.to(device)\n",
    "            trg = trg.to(device)\n",
    "            output = model(src, trg, 0) #turn off teacher forcing\n",
    "\n",
    "            #trg = [trg len, batch size]\n",
    "            #output = [trg len, batch size, output dim]\n",
    "\n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            trg = trg[1:].reshape(-1)\n",
    "\n",
    "            #trg = [(trg len - 1) * batch size]\n",
    "            #output = [(trg len - 1) * batch size, output dim]\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3374481074228273 5.297865208807021\n",
      "1.3136368889902812 5.292362336429108\n",
      "1.2797559591375387 5.313594293000171\n",
      "1.2409358961962174 5.285190923191677\n",
      "1.2147206622891769 5.277388626169936\n",
      "1.198586932778863 5.287266259624208\n",
      "1.1775167363739147 5.319936996679811\n",
      "1.1601882145250129 5.404023858988396\n",
      "1.1440086207537792 5.487968570346773\n",
      "1.1331019845012213 5.3515930539722385\n",
      "1.1173553513875296 5.2927597660884675\n",
      "1.1030308015339803 5.413021916540984\n",
      "1.0928088345463758 5.3956130055995\n",
      "1.0781456292600322 5.399575396861614\n",
      "1.0654458912622777 5.3620887352298725\n",
      "1.055238149734411 5.356019328316424\n",
      "1.0445925931752316 5.428011049734097\n",
      "1.0353394867839194 5.422454022915564\n",
      "1.026464333024785 5.4612542647067635\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-129bbae90ce2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mtr_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCLIP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mv_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-41-b741842fe824>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, iterator, optimizer, criterion, clip)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/w266_env/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/w266_env/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 100\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "train_loss = []\n",
    "valid_loss = []\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    tr_loss = train(model, train_set.loader, optimizer, criterion, CLIP)\n",
    "    train_loss.append(tr_loss)\n",
    "    v_loss = evaluate(model, test_set.loader, criterion)\n",
    "    valid_loss.append(v_loss)\n",
    "    lr_scheduler.step()\n",
    "    print(tr_loss, v_loss)\n",
    "    if(best_valid_loss > v_loss):\n",
    "        best_valid_loss = v_loss\n",
    "        save_file = './base_model.pt'\n",
    "        torch.save(model.state_dict(), save_file)\n",
    "print(train_loss, valid_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.shape\n",
    "a = outputs.argmax(-1)\n",
    "a1 = a.transpose(1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "place in the morning of the french army when they were all right and a\n",
      "platon karataev of whom he had heard from pierre his love for natasha\n",
      "------------------\n",
      "who had been at the same time he was an air of a man who was already an\n",
      "whole face as she had so often seen it in the stillness of the night\n",
      "------------------\n",
      "have you heard of the same i shall be in a man who was a single and\n",
      "has lived through\n",
      "------------------\n",
      "french soldiers were silent and silently still his head and heard\n",
      "freedom\n",
      "------------------\n",
      "a commander of the rostovs were standing the french and the soldiers\n",
      "a happiness he had long forgotten and of which he had not even been\n",
      "------------------\n",
      "what a story of the emperor alexander in moscow and what is it asked th\n",
      "what pierre had told them princess mary did not express her opinion of\n",
      "------------------\n",
      "speaking of his face and the french were already been able to sing and\n",
      "special enclosure where there are oatsthat this very ram swelling\n",
      "------------------\n",
      "authority and the same in the middle of the russian army with a smile a\n",
      "autumn and which dessalles called les fils de la vierge in front was\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "t = trg.transpose(1,0)\n",
    "a = a1.cpu()\n",
    "b = t.cpu()\n",
    "b = b.detach().numpy()\n",
    "a = a.detach().numpy()\n",
    "for i in range(a.shape[0]):\n",
    "   print(decode_labels(a[i,:]))\n",
    "   print(decode_labels(b[i,:]))\n",
    "   print(\"------------------\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, batch in enumerate(train_set.loader):\n",
    "      src,trg = batch\n",
    "      if(idx == 0):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0,  1,  4,  4, 22,  1, 14, 20, 20,  1,  1,  7,  7,  5,  5, 15, 21, 21,\n",
      "        19, 28, 19, 20, 20, 25, 12, 12,  5,  5, 28, 12,  9, 11, 11,  5,  5, 28,\n",
      "         1, 28, 23,  5, 12, 12, 12, 12,  7,  1,  1, 18, 14, 14,  9,  9, 19, 19,\n",
      "         8,  5,  5,  4,  4, 28, 10, 10, 15,  9, 14, 20, 20, 28, 15,  6, 28, 18,\n",
      "        15,  1,  1, 19, 20, 20, 28,  2,  5,  5,  5,  6, 28, 15, 14, 28,  1,  1,\n",
      "        28,  8, 15, 20, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 31])\n",
      "tensor([ 0, 20, 20,  8,  5,  5, 28, 18, 15, 15, 15, 13, 13, 28, 19, 20,  5,  5,\n",
      "        16, 16, 16,  9,  9, 14, 14,  7, 28,  2,  2, 18,  9,  9, 19, 11, 12, 25,\n",
      "        28, 20, 20, 15, 28, 20,  8,  5, 28,  6, 18, 18, 15, 15, 14, 20, 28, 15,\n",
      "         6, 28, 20, 20,  8,  8,  5, 28,  3,  3, 18, 15, 15, 23,  4, 28, 15, 15,\n",
      "         6, 28,  7,  5,  5, 14, 14, 20, 20, 18, 25, 25, 30, 30, 30, 30, 30, 30,\n",
      "        30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 31])\n"
     ]
    }
   ],
   "source": [
    "for i in range(2):\n",
    "   print(src[i])\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.extend([2])\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [i for i in range(6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.95**(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "transducer-example.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

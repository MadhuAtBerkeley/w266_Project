{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BbwgR5UdNkkm"
   },
   "source": [
    "# Transducer ASR implementation in PyTorch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q-iHU02C7fAj",
    "outputId": "0ffff827-7fb1-42a4-95d3-6a9fb3325e04"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import string\n",
    "import unidecode\n",
    "import numpy as np\n",
    "import itertools\n",
    "from itertools import cycle, islice\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import BufferedShuffleDataset, IterableDataset, DataLoader\n",
    "from copy import deepcopy\n",
    "softmax_mod = torch.nn.Softmax(dim=0)\n",
    "import jiwer\n",
    "from jiwer import wer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transducer Module\n",
    "\n",
    "\n",
    "<img src=\"trans_module.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "B7mLFyUG7kJH"
   },
   "outputs": [],
   "source": [
    "NULL_INDEX = 0\n",
    "\n",
    "encoder_dim = 1024\n",
    "embedding_dim = 32\n",
    "predictor_dim = 1024\n",
    "joiner_dim = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "KE7j2T5EY33-"
   },
   "outputs": [],
   "source": [
    "class TransEncoder(torch.nn.Module):\n",
    "  def __init__(self, num_inputs):\n",
    "    super(TransEncoder, self).__init__()\n",
    "    self.embed = torch.nn.Embedding(num_inputs, embedding_dim)\n",
    "    self.rnn = torch.nn.GRU(input_size=embedding_dim, hidden_size=encoder_dim, num_layers=3, batch_first=True, bidirectional=True, dropout=0.1)\n",
    "    self.linear = torch.nn.Linear(encoder_dim*2, joiner_dim)\n",
    "\n",
    "  def forward(self, x):\n",
    "    out = x\n",
    "    out = self.embed(out)\n",
    "    out = self.rnn(out)[0]\n",
    "    out = self.linear(out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "hPARF5LmY7-r"
   },
   "outputs": [],
   "source": [
    "class TransPredictor(torch.nn.Module):\n",
    "  def __init__(self, num_outputs):\n",
    "    super(TransPredictor, self).__init__()\n",
    "    self.embed = torch.nn.Embedding(num_outputs, embedding_dim)\n",
    "    self.rnn = torch.nn.GRUCell(input_size=embedding_dim, hidden_size=predictor_dim)\n",
    "    self.linear = torch.nn.Linear(predictor_dim, joiner_dim)\n",
    "    \n",
    "    self.initial_state = torch.nn.Parameter(torch.randn(predictor_dim))\n",
    "    self.start_symbol = NULL_INDEX\n",
    "\n",
    "  def forward_one_step(self, input, previous_state):\n",
    "    embedding = self.embed(input)\n",
    "    state = self.rnn.forward(embedding, previous_state)\n",
    "    out = self.linear(state)\n",
    "    return out, state\n",
    "\n",
    "  def forward(self, y):\n",
    "    batch_size = y.shape[0]\n",
    "    U = y.shape[1]\n",
    "    outs = []\n",
    "    state = torch.stack([self.initial_state] * batch_size).to(y.device)\n",
    "    for u in range(U+1): # need U+1 to get null output for final timestep \n",
    "      if u == 0:\n",
    "        decoder_input = torch.tensor([self.start_symbol] * batch_size).to(y.device)\n",
    "      else:\n",
    "        decoder_input = y[:,u-1]\n",
    "      out, state = self.forward_one_step(decoder_input, state)\n",
    "      outs.append(out)\n",
    "    out = torch.stack(outs, dim=1)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Vlzca1orZDLa"
   },
   "outputs": [],
   "source": [
    "class TransJoiner(torch.nn.Module):\n",
    "  def __init__(self, num_outputs):\n",
    "    super(TransJoiner, self).__init__()\n",
    "    self.linear = torch.nn.Linear(joiner_dim, num_outputs)\n",
    "\n",
    "  def forward(self, encoder_out, predictor_out):\n",
    "    out = encoder_out + predictor_out\n",
    "    out = torch.nn.functional.relu(out)\n",
    "    out = self.linear(out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "sYSagKi-gHM4"
   },
   "outputs": [],
   "source": [
    "class TransducerASR(torch.nn.Module):\n",
    "  def __init__(self, num_inputs, num_outputs):\n",
    "    super(TransducerASR, self).__init__()\n",
    "    self.encoder = TransEncoder(num_inputs)\n",
    "    self.predictor = TransPredictor(num_outputs)\n",
    "    self.joiner = TransJoiner(num_outputs)\n",
    "\n",
    "    if torch.cuda.is_available(): self.device = \"cuda\"\n",
    "    else: self.device = \"cpu\"\n",
    "    self.to(self.device)\n",
    "\n",
    "  def compute_forward_prob(self, joiner_out, T, U, y):\n",
    "    \"\"\"\n",
    "    joiner_out: tensor of shape (B, T_max, U_max+1, #labels)\n",
    "    T: list of input lengths\n",
    "    U: list of output lengths \n",
    "    y: label tensor (B, U_max+1)\n",
    "    \"\"\"\n",
    "    B = joiner_out.shape[0]\n",
    "    T_max = joiner_out.shape[1]\n",
    "    U_max = joiner_out.shape[2] - 1\n",
    "    log_alpha = torch.zeros(B, T_max, U_max+1).to(y.device)\n",
    "    for t in range(T_max):\n",
    "      for u in range(U_max+1):\n",
    "          if u == 0:\n",
    "            if t == 0:\n",
    "              log_alpha[:, t, u] = 0.\n",
    "\n",
    "            else: #t > 0\n",
    "              log_alpha[:, t, u] = log_alpha[:, t-1, u] + joiner_out[:, t-1, 0, NULL_INDEX] \n",
    "                  \n",
    "          else: #u > 0\n",
    "            if t == 0:\n",
    "              log_alpha[:, t, u] = log_alpha[:, t,u-1] + torch.gather(joiner_out[:, t, u-1], dim=1, index=y[:,u-1].view(-1,1) ).reshape(-1)\n",
    "            \n",
    "            else: #t > 0\n",
    "              log_alpha[:, t, u] = torch.logsumexp(torch.stack([\n",
    "                  log_alpha[:, t-1, u] + joiner_out[:, t-1, u, NULL_INDEX],\n",
    "                  log_alpha[:, t, u-1] + torch.gather(joiner_out[:, t, u-1], dim=1, index=y[:,u-1].view(-1,1) ).reshape(-1)\n",
    "              ]), dim=0)\n",
    "    \n",
    "    log_probs = []\n",
    "    for b in range(B):\n",
    "      log_prob = log_alpha[b, T[b]-1, U[b]] + joiner_out[b, T[b]-1, U[b], NULL_INDEX]\n",
    "      log_probs.append(log_prob)\n",
    "    log_probs = torch.stack(log_probs) \n",
    "    return log_prob\n",
    "\n",
    "  def compute_loss(self, x, y, T, U):\n",
    "    encoder_out = self.encoder.forward(x)\n",
    "    predictor_out = self.predictor.forward(y)\n",
    "    joiner_out = self.joiner.forward(encoder_out.unsqueeze(2), predictor_out.unsqueeze(1)).log_softmax(3)\n",
    "    loss = -self.compute_forward_prob(joiner_out, T, U, y).mean()\n",
    "    return loss\n",
    "\n",
    "  def greedy_search(self, x, T):\n",
    "    y_batch = []\n",
    "    B = len(x)\n",
    "    encoder_out = self.encoder.forward(x)\n",
    "    U_max = 200\n",
    "    for b in range(B):\n",
    "       t = 0; u = 0; y = [self.predictor.start_symbol]; predictor_state = self.predictor.initial_state.unsqueeze(0)\n",
    "       path_metric = 1\n",
    "       while t < T[b] and u < U_max:\n",
    "         predictor_input = torch.tensor([ y[-1] ]).to(x.device)\n",
    "         g_u, predictor_state = self.predictor.forward_one_step(predictor_input, predictor_state)\n",
    "         f_t = encoder_out[b, t]\n",
    "         h_t_u = self.joiner.forward(f_t, g_u)\n",
    "        \n",
    "         argmax = h_t_u.max(-1)[1].item()\n",
    "    \n",
    "         if argmax == NULL_INDEX:\n",
    "           t += 1\n",
    "         else: # argmax == a label\n",
    "           u += 1\n",
    "           y.append(argmax)\n",
    "       y_batch.append(y[1:]) # remove blank symbol\n",
    "    \n",
    "    return y_batch\n",
    "\n",
    "  def beam_search(self, x, T):  \n",
    "        \n",
    "    def continue_search(t, u, T_max, U_max, k):\n",
    "      ret_val = True\n",
    "      for i in range(k):\n",
    "          ret_val = ret_val and (t[i]<T_max and u[i] < U_max)\n",
    "      return(ret_val)\n",
    "  \n",
    "    k = 2\n",
    "    B = len(x)\n",
    "    encoder_out = self.encoder.forward(x)\n",
    "    U_max = 200\n",
    "    y_beam_batch = []\n",
    "    for b in range(B):\n",
    "       first_beam = True    \n",
    "       t =[0]*k; u = [0]*k; \n",
    "       y = [self.predictor.start_symbol]*k; pred_state = [self.predictor.initial_state.unsqueeze(0)]*k\n",
    "       path_sequence = [[self.predictor.start_symbol] for i in range(k)] \n",
    "       path_metric = [1]*k\n",
    "       temp_sequence = [[0]]*k\n",
    "       temp_t = [0]*k\n",
    "       temp_u = [0]*k\n",
    "       while continue_search(t, u, T[b], U_max, k): # t[beam_index] < T[b] and u[beam_index] < U_max:\n",
    "          temp_path_m = np.array([])\n",
    "          argmax = []  \n",
    "          temp_pred_state = []  \n",
    "          num_beam = k\n",
    "          if(first_beam):\n",
    "             num_beam = 1\n",
    "             first_beam = False   \n",
    "        \n",
    "          for beam_index in range(num_beam):    \n",
    "            predictor_input = torch.tensor([ y[beam_index] ]).to(x.device)\n",
    "            g_u, beam_state = self.predictor.forward_one_step(predictor_input, pred_state[beam_index])\n",
    "            f_t = encoder_out[b, t[beam_index]]\n",
    "            h_t_u = self.joiner.forward(f_t, g_u) \n",
    "            h_t_u = torch.sort(h_t_u) \n",
    "            soft_out = softmax_mod(h_t_u[0][0])  \n",
    "            argmax.extend([ h_t_u[1][0][-i].item() for i in range(1, k+1)])\n",
    "            temp_path_m = np.append(temp_path_m , [path_metric[beam_index]*(soft_out[-i].item()) for i in range(1, k+1)])\n",
    "            temp_pred_state.append(beam_state)\n",
    "      \n",
    "     \n",
    "          args = temp_path_m.argsort()[::-1][:k]\n",
    "          beam_args = args//k\n",
    "          #print(argmax, args, beam_args, temp_path_m[args]) \n",
    "          #print(decode_labels(argmax))\n",
    "          for beam_index in range(k):\n",
    "            path_metric[beam_index] = temp_path_m[args[beam_index]]\n",
    "            temp_sequence[beam_index] = deepcopy(path_sequence[beam_args[beam_index]])  \n",
    "            temp_t[beam_index] = t[beam_args[beam_index]]  \n",
    "            temp_u[beam_index] = u[beam_args[beam_index]] \n",
    "            out_pred = argmax[args[beam_index]]  \n",
    "            #print(\"Curr Seq:\", decode_labels(temp_sequence[beam_index]), \"Char:\", decode_labels([out_pred]))  \n",
    "            pred_state[beam_index] = temp_pred_state[beam_args[beam_index]]\n",
    "            if out_pred == NULL_INDEX:\n",
    "              temp_t[beam_index] += 1\n",
    "            else: # argmax == a label\n",
    "              temp_u[beam_index] += 1\n",
    "              y[beam_index] = out_pred\n",
    "              temp_sequence[beam_index].append(out_pred) \n",
    "               \n",
    "          for beam_index in range(k): \n",
    "             path_sequence[beam_index] = temp_sequence[beam_index]\n",
    "             t[beam_index] = temp_t[beam_index]  \n",
    "             u[beam_index] = temp_u[beam_index] \n",
    "          #print(\"Next Seq:\", decode_labels(path_sequence[beam_index]))  \n",
    "          \n",
    "      #print('-------------------------------')  \n",
    "    #h_t_u = torch.sort(h_t_u) \n",
    "       y_beam_batch.append(path_sequence[0][1:])\n",
    "    return y_beam_batch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IK0c2S2xaARd"
   },
   "source": [
    "\n",
    "\n",
    "<img src=\"alignments.png\" width=\"75%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "V0xeyb7Jf18_"
   },
   "outputs": [],
   "source": [
    "def greedy_search(self, x, T):\n",
    "  y_batch = []\n",
    "  B = len(x)\n",
    "  k = 10 \n",
    "  encoder_out = self.encoder.forward(x)\n",
    "  U_max = 200\n",
    "  for b in range(B):\n",
    "    t = 0; u = 0; y = [self.predictor.start_symbol]; predictor_state = self.predictor.initial_state.unsqueeze(0)\n",
    "    path_metric = 1\n",
    "    while t < T[b] and u < U_max:\n",
    "     \n",
    "      predictor_input = torch.tensor([ y[-1] ]).to(x.device)\n",
    "      g_u, predictor_state = self.predictor.forward_one_step(predictor_input, predictor_state)\n",
    "      f_t = encoder_out[b, t]\n",
    "      h_t_u = self.joiner.forward(f_t, g_u)\n",
    "        \n",
    "      argmax = h_t_u.max(-1)[1].item()\n",
    "    \n",
    "      if argmax == NULL_INDEX:\n",
    "        t += 1\n",
    "      else: # argmax == a label\n",
    "        u += 1\n",
    "        y.append(argmax)\n",
    "    y_batch.append(y[1:]) # remove start symbol\n",
    "    \n",
    "  return y_batch\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ff9raB0jVGzN"
   },
   "source": [
    "# Utilities - Dataloader and labels\n",
    "\n",
    "Here we will add a bit of boilerplate code for training and loading data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5b17OQm4WdVy",
    "outputId": "8d63de0a-a3de-477a-ac07-e0c84431fb4f"
   },
   "outputs": [],
   "source": [
    "char_labels =  [\" \", \"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\", \"k\", \"l\", \"m\", \"n\", \n",
    "               \"o\", \"p\", \"q\", \"r\", \"s\", \"t\", \"u\", \"v\", \"w\", \"x\", \"y\", \"z\", \"'\"]\n",
    "\n",
    "char_dict  =  {char_labels[i] : i for i in range(len(char_labels))} \n",
    "inv_char_dict = {i : char_labels[i] for i in range(len(char_labels))} \n",
    "\n",
    "class IterableDataSetWithShuffle(IterableDataset):\n",
    "    def __init__(self, filename, max_len=10):\n",
    "        self.filename = filename\n",
    "        self.max_len = max_len\n",
    "        \n",
    "        \n",
    "    def parse_file(self, filename):\n",
    "        \n",
    "        def gen_features(line):\n",
    "            \n",
    "            #Remove linefeed and convert to lower case\n",
    "            line = line.replace(\"\\n\", \"\") \n",
    "            line = unidecode.unidecode(line)\n",
    "            line = line.lower()\n",
    "            \n",
    "            #Randomly repeat characters to mimic the phonemes in speech segment\n",
    "            #Randomly remove characters to simulate auditory effect \n",
    "            line_y = \"\".join(c for c in line if c in char_labels)\n",
    "            line_x = \"\".join(c*random.randint(0,1) if c in (\"gjp\") else c*random.randint(1,2) for c in line_y)   \n",
    "            line_x = \" \".join(line_x.split())\n",
    "            return((line_x,line_y)) \n",
    "        \n",
    "        with open(filename, 'r') as file_obj:\n",
    "             \n",
    "             #restrict length of sentences to max_len words   \n",
    "             for line in file_obj:\n",
    "                short_t_lines = []\n",
    "                line = line.split('\\t')[2]\n",
    "                temp = line.split(' ')\n",
    "                count = len(temp)//self.max_len\n",
    "                resid = len(temp)%self.max_len\n",
    "    \n",
    "                k = 0\n",
    "                while(count > 0):    \n",
    "                     short_line = ' '.join(temp[k:k+self.max_len])  \n",
    "  \n",
    "                     short_t_lines.append(gen_features(short_line))\n",
    "                     count = count - 1\n",
    "                     k = k + self.max_len\n",
    "                if(resid > 0):\n",
    "                     short_line = ' '.join(temp[k:k+resid])\n",
    "                     short_line = ' '.join(short_line.split())\n",
    "                     if(len(short_line)>1):\n",
    "                        short_t_lines.append(gen_features(short_line))\n",
    "                 \n",
    "                yield  from short_t_lines   \n",
    "                    \n",
    "    def __iter__(self):\n",
    "        return cycle(self.parse_file(self.filename))  \n",
    "    \n",
    "    \n",
    "#Encode and decode functions to map characters to int vectors\n",
    "# 0 is mapped to blank symbol used for alignments\n",
    "\n",
    "def encode_string(s):\n",
    "  return [char_dict[c] + 1 for c in s]\n",
    "\n",
    "def decode_labels(l):\n",
    "  return \"\".join([inv_char_dict[c - 1] for c in l])\n",
    "\n",
    "\n",
    "class Collate:\n",
    "  def __call__(self, batch):\n",
    "    \"\"\"\n",
    "    batch: list of tuples (input string, output string)\n",
    "    Returns a minibatch of strings, encoded as labels and padded to have the same length.\n",
    "    \"\"\"\n",
    "    x = []; y = []\n",
    "    batch_size = len(batch)\n",
    "    for index in range(batch_size):\n",
    "      x_,y_ = batch[index]\n",
    "      x.append(encode_string(x_))\n",
    "      y.append(encode_string(y_))\n",
    "\n",
    "    # pad all sequences to have same length\n",
    "    T = [len(x_) for x_ in x]\n",
    "    U = [len(y_) for y_ in y]\n",
    "    T_max = max(T)\n",
    "    U_max = max(U)\n",
    "    for index in range(batch_size):\n",
    "      x[index] += [NULL_INDEX] * (T_max - len(x[index]))\n",
    "      x[index] = torch.tensor(x[index])\n",
    "      y[index] += [NULL_INDEX] * (U_max - len(y[index]))\n",
    "      y[index] = torch.tensor(y[index])\n",
    "\n",
    "    # stack into single tensor\n",
    "    x = torch.stack(x)\n",
    "    y = torch.stack(y)\n",
    "    T = torch.tensor(T)\n",
    "    U = torch.tensor(U)\n",
    "\n",
    "    return (x,y,T,U)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"../LibriSpeech/train-clean-360/transcripts.tsv\"\n",
    "train_filename = \"../LibriSpeech/train-clean-100/transcripts_460.tsv\"\n",
    "eval_filename = \"../LibriSpeech/dev-clean/transcripts.tsv\"\n",
    "test_filename = \"../LibriSpeech/test-clean/transcripts.tsv\"\n",
    "collate = Collate()\n",
    "\n",
    "train_dataset = IterableDataSetWithShuffle(train_filename, max_len=10)\n",
    "train_buffer = BufferedShuffleDataset(train_dataset, buffer_size=1024)\n",
    "train_loader = DataLoader(train_buffer, batch_size=32, collate_fn=collate)     \n",
    "\n",
    "eval_dataset = IterableDataSetWithShuffle(eval_filename, max_len=10)\n",
    "eval_buffer = BufferedShuffleDataset(eval_dataset, buffer_size=1024)\n",
    "eval_loader = DataLoader(eval_buffer, batch_size=8, collate_fn=collate)     \n",
    "\n",
    "test_dataset = IterableDataSetWithShuffle(test_filename, max_len=10)\n",
    "test_buffer = BufferedShuffleDataset(test_dataset, buffer_size=1024)\n",
    "test_loader = DataLoader(test_buffer, batch_size=8, collate_fn=collate)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "gaZEQYzfFEQ0"
   },
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "  def __init__(self, model, lr=0.0001, dr=0.9):\n",
    "    self.model = model\n",
    "    self.lr = lr\n",
    "    self.optimizer = torch.optim.Adam(model.parameters(), lr=self.lr)\n",
    "    self.dr = dr\n",
    "    self.lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=self.optimizer, gamma=self.dr)\n",
    "  \n",
    "  def train(self, dataset, print_interval=100):\n",
    "    train_loss = 0\n",
    "    num_samples = 0\n",
    "    self.model.train()\n",
    "    num_batch = 9200\n",
    "    for idx, batch in enumerate(islice(dataset, num_batch)):\n",
    "      x,y,T,U = batch\n",
    "      x = x.to(self.model.device); y = y.to(self.model.device)\n",
    "      batch_size = len(x)\n",
    "      num_samples += batch_size\n",
    "      loss = self.model.compute_loss(x,y,T,U)\n",
    "      self.optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      self.optimizer.step()\n",
    "      train_loss += loss.item() * batch_size\n",
    "      if idx % print_interval == 0:\n",
    "         print(\"Batch_num :\", idx)\n",
    "    train_loss /= num_samples\n",
    "    return train_loss\n",
    "\n",
    "\n",
    "  def test(self, dataset, print_interval=100):\n",
    "    test_loss = 0\n",
    "    num_samples = 0\n",
    "    self.model.eval()\n",
    "    num_batch = 800\n",
    "    for idx, batch in enumerate(islice(dataset, num_batch)):\n",
    "      x,y,T,U = batch\n",
    "      xx = x.to(self.model.device); yy = y.to(self.model.device)\n",
    "      batch_size = len(x)\n",
    "      num_samples += batch_size\n",
    "      loss = self.model.compute_loss(xx,yy,T,U)\n",
    "      test_loss += loss.item() * batch_size\n",
    "      if idx % print_interval == 0:\n",
    "        print(\"\\n\")\n",
    "        print(\"input:\", decode_labels(x[0,:T[0]].numpy()))\n",
    "        print(\"guess:\", decode_labels(self.model.greedy_search(xx,T)[0]))\n",
    "        print(\"truth:\", decode_labels(y[0,:U[0]].numpy()))\n",
    "        print(\"\")\n",
    "    test_loss /= num_samples\n",
    "    return test_loss\n",
    "\n",
    "  def fit(self, train_dataset, eval_dataset, num_epochs=10, print_interval = 100):\n",
    "    train_loss = []\n",
    "    eval_loss = []\n",
    "    max_eval_loss = float('inf')\n",
    "  \n",
    "    for epochs in range(num_epochs):\n",
    "        print(\"Epoch No:{}\".format(epochs))\n",
    "        t_loss = self.train(train_dataset, print_interval)\n",
    "        train_loss.append(t_loss)\n",
    "        self.lr_scheduler.step()\n",
    "        e_loss = self.test(eval_dataset, print_interval)\n",
    "        eval_loss.append(e_loss)\n",
    "        print(\"train loss {:.2f} eval_loss {:.2f}\".format(t_loss, e_loss))\n",
    "         \n",
    "        if(e_loss < max_eval_loss):\n",
    "           max_eval_loss = e_loss\n",
    "           save_file = \"./model_epoch_{}.pt\".format(epochs)\n",
    "           torch.save(model.state_dict(), save_file)\n",
    "        \n",
    "    return train_loss, eval_loss\n",
    "\n",
    "\n",
    " \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tester:\n",
    "  def __init__(self, model, search='greedy'):\n",
    "     self.model = model\n",
    "     if(search=='beam'):\n",
    "        self.search_fn = self.model.beam_search\n",
    "     else:\n",
    "        self.search_fn = self.model.greedy_search\n",
    "\n",
    "  def test(self, dataset):\n",
    "    self.model.eval()\n",
    "    y_pred = []\n",
    "    y_test = []\n",
    "    num_batch = 800\n",
    "    for idx, batch in enumerate(islice(dataset, num_batch)):\n",
    "      x,y,T,U = batch\n",
    "      xx = x.to(self.model.device); yy = y.to(self.model.device)\n",
    "      y_batch = self.search_fn(xx,T)\n",
    "      for i in range(len(y_batch)):\n",
    "          y_pred.append(decode_labels(y_batch[i]))  \n",
    "          y_test.append(decode_labels(y[i,:U[i]].numpy()))\n",
    "    return y_test, y_pred\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q4PupgBKWe6p"
   },
   "source": [
    "# Training the model\n",
    "\n",
    "Now we will train a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1TSrbH9xGPEC",
    "outputId": "bd12ead0-530e-489f-a15c-cd649ff59820"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch No:0\n",
      "Batch_num : 0\n",
      "Batch_num : 100\n",
      "Batch_num : 200\n",
      "Batch_num : 300\n",
      "Batch_num : 400\n",
      "Batch_num : 500\n",
      "Batch_num : 600\n",
      "Batch_num : 700\n",
      "Batch_num : 800\n",
      "Batch_num : 900\n",
      "Batch_num : 1000\n",
      "Batch_num : 1100\n",
      "Batch_num : 1200\n",
      "Batch_num : 1300\n",
      "Batch_num : 1400\n",
      "Batch_num : 1500\n",
      "Batch_num : 1600\n",
      "Batch_num : 1700\n",
      "Batch_num : 1800\n",
      "Batch_num : 1900\n",
      "Batch_num : 2000\n",
      "Batch_num : 2100\n",
      "Batch_num : 2200\n",
      "Batch_num : 2300\n",
      "Batch_num : 2400\n",
      "Batch_num : 2500\n",
      "Batch_num : 2600\n",
      "Batch_num : 2700\n",
      "Batch_num : 2800\n",
      "Batch_num : 2900\n",
      "Batch_num : 3000\n",
      "Batch_num : 3100\n",
      "Batch_num : 3200\n",
      "Batch_num : 3300\n",
      "Batch_num : 3400\n",
      "Batch_num : 3500\n",
      "Batch_num : 3600\n",
      "Batch_num : 3700\n",
      "Batch_num : 3800\n",
      "Batch_num : 3900\n",
      "Batch_num : 4000\n",
      "Batch_num : 4100\n",
      "Batch_num : 4200\n",
      "Batch_num : 4300\n",
      "Batch_num : 4400\n",
      "Batch_num : 4500\n",
      "Batch_num : 4600\n",
      "Batch_num : 4700\n",
      "Batch_num : 4800\n",
      "Batch_num : 4900\n",
      "Batch_num : 5000\n",
      "Batch_num : 5100\n",
      "Batch_num : 5200\n",
      "Batch_num : 5300\n",
      "Batch_num : 5400\n",
      "Batch_num : 5500\n",
      "Batch_num : 5600\n",
      "Batch_num : 5700\n",
      "Batch_num : 5800\n",
      "Batch_num : 5900\n",
      "Batch_num : 6000\n",
      "Batch_num : 6100\n",
      "Batch_num : 6200\n",
      "Batch_num : 6300\n",
      "Batch_num : 6400\n",
      "Batch_num : 6500\n",
      "Batch_num : 6600\n",
      "Batch_num : 6700\n",
      "Batch_num : 6800\n",
      "Batch_num : 6900\n",
      "Batch_num : 7000\n",
      "Batch_num : 7100\n",
      "Batch_num : 7200\n",
      "Batch_num : 7300\n",
      "Batch_num : 7400\n",
      "Batch_num : 7500\n",
      "Batch_num : 7600\n",
      "Batch_num : 7700\n",
      "Batch_num : 7800\n",
      "Batch_num : 7900\n",
      "Batch_num : 8000\n",
      "Batch_num : 8100\n",
      "Batch_num : 8200\n",
      "Batch_num : 8300\n",
      "Batch_num : 8400\n",
      "Batch_num : 8500\n",
      "Batch_num : 8600\n",
      "Batch_num : 8700\n",
      "Batch_num : 8800\n",
      "Batch_num : 8900\n",
      "Batch_num : 9000\n",
      "Batch_num : 9100\n",
      "\n",
      "\n",
      "input: ccome in annd sstoop withh mee aand nno haarmm shhaall\n",
      "guess: come in and stop with me and no har shal\n",
      "truth: come in and stop with me and no harm shall\n",
      "\n",
      "\n",
      "\n",
      "input: oo qquuiieettllyy aaloone nno haarrmm wwilll beefalll yyoou\n",
      "guess: o quietly alone no har wil befal you\n",
      "truth: go quietly alone no harm will befall you\n",
      "\n",
      "\n",
      "\n",
      "input: ssuummer desscceendin occcasionallly iintoo thhe aaddaacent ssaaee pllaainss andd lavaa\n",
      "guess: sumer descending ocasionaly into the adacent sage plains and lava\n",
      "truth: summer descending occasionally into the adjacent sage plains and lava\n",
      "\n",
      "\n",
      "\n",
      "input: ii sshall lloocck uup aall thhe dooorss aandd wwinddowwss iinn\n",
      "guess: i shal lock up al the dors and windows in\n",
      "truth: i shall lock up all the doors and windows in\n",
      "\n",
      "\n",
      "\n",
      "input: eeduuccatiinng off hhiis wwooodlaandd bbriide aand iitt wwaass thhee aambitioon\n",
      "guess: educating of his wodland bride and it was the ambition\n",
      "truth: educating of his woodland bride and it was the ambition\n",
      "\n",
      "\n",
      "\n",
      "input: guaardd itt nnoww\n",
      "guess: gard it now\n",
      "truth: guard it now\n",
      "\n",
      "\n",
      "\n",
      "input: hheers hhas bbeenn prrooddigiouuss\n",
      "guess: hers has ben prodigious\n",
      "truth: hers has been prodigious\n",
      "\n",
      "\n",
      "\n",
      "input: nnoott that ttheey suuposeedd tthee siritt ccooulld uusee tthe immpleemmeennttss\n",
      "guess: not that they suposed the spirit could use the implements\n",
      "truth: not that they supposed the spirit could use the implements\n",
      "\n",
      "train loss 0.87 eval_loss 0.95\n",
      "Epoch No:1\n",
      "Batch_num : 0\n",
      "Batch_num : 100\n",
      "Batch_num : 200\n",
      "Batch_num : 300\n",
      "Batch_num : 400\n",
      "Batch_num : 500\n",
      "Batch_num : 600\n",
      "Batch_num : 700\n",
      "Batch_num : 800\n",
      "Batch_num : 900\n",
      "Batch_num : 1000\n",
      "Batch_num : 1100\n",
      "Batch_num : 1200\n",
      "Batch_num : 1300\n",
      "Batch_num : 1400\n",
      "Batch_num : 1500\n",
      "Batch_num : 1600\n",
      "Batch_num : 1700\n",
      "Batch_num : 1800\n",
      "Batch_num : 1900\n",
      "Batch_num : 2000\n",
      "Batch_num : 2100\n",
      "Batch_num : 2200\n",
      "Batch_num : 2300\n",
      "Batch_num : 2400\n",
      "Batch_num : 2500\n",
      "Batch_num : 2600\n",
      "Batch_num : 2700\n",
      "Batch_num : 2800\n",
      "Batch_num : 2900\n",
      "Batch_num : 3000\n",
      "Batch_num : 3100\n",
      "Batch_num : 3200\n",
      "Batch_num : 3300\n",
      "Batch_num : 3400\n",
      "Batch_num : 3500\n",
      "Batch_num : 3600\n",
      "Batch_num : 3700\n",
      "Batch_num : 3800\n",
      "Batch_num : 3900\n",
      "Batch_num : 4000\n",
      "Batch_num : 4100\n",
      "Batch_num : 4200\n",
      "Batch_num : 4300\n",
      "Batch_num : 4400\n",
      "Batch_num : 4500\n",
      "Batch_num : 4600\n",
      "Batch_num : 4700\n",
      "Batch_num : 4800\n",
      "Batch_num : 4900\n",
      "Batch_num : 5000\n",
      "Batch_num : 5100\n",
      "Batch_num : 5200\n",
      "Batch_num : 5300\n",
      "Batch_num : 5400\n",
      "Batch_num : 5500\n",
      "Batch_num : 5600\n",
      "Batch_num : 5700\n",
      "Batch_num : 5800\n",
      "Batch_num : 5900\n",
      "Batch_num : 6000\n",
      "Batch_num : 6100\n",
      "Batch_num : 6200\n",
      "Batch_num : 6300\n",
      "Batch_num : 6400\n",
      "Batch_num : 6500\n",
      "Batch_num : 6600\n",
      "Batch_num : 6700\n",
      "Batch_num : 6800\n",
      "Batch_num : 6900\n",
      "Batch_num : 7000\n",
      "Batch_num : 7100\n",
      "Batch_num : 7200\n",
      "Batch_num : 7300\n",
      "Batch_num : 7400\n",
      "Batch_num : 7500\n",
      "Batch_num : 7600\n",
      "Batch_num : 7700\n",
      "Batch_num : 7800\n",
      "Batch_num : 7900\n",
      "Batch_num : 8000\n",
      "Batch_num : 8100\n",
      "Batch_num : 8200\n",
      "Batch_num : 8300\n",
      "Batch_num : 8400\n",
      "Batch_num : 8500\n",
      "Batch_num : 8600\n",
      "Batch_num : 8700\n",
      "Batch_num : 8800\n",
      "Batch_num : 8900\n",
      "Batch_num : 9000\n",
      "Batch_num : 9100\n",
      "\n",
      "\n",
      "input: men when theey mighht riisse to tthhee raankk of goddss\n",
      "guess: men when they might rise to the rank of gods\n",
      "truth: men when they might rise to the rank of gods\n",
      "\n",
      "\n",
      "\n",
      "input: bbrreaasstt\n",
      "guess: brest\n",
      "truth: breast\n",
      "\n",
      "\n",
      "\n",
      "input: o quuietllyy alloone nno hharrm wiilll befall youu\n",
      "guess: o quietly alone no har wil befal you\n",
      "truth: go quietly alone no harm will befall you\n",
      "\n",
      "\n",
      "\n",
      "input: tthe oolld genttllewwoommann wass sittting inn oonnee off thee ssttraaiiht\n",
      "guess: the old gentlewoman was sitin in one of the straight\n",
      "truth: the old gentlewoman was sitting in one of the straight\n",
      "\n",
      "\n",
      "\n",
      "input: wwiitthh us\n",
      "guess: with us\n",
      "truth: with us\n",
      "\n",
      "\n",
      "\n",
      "input: modde ut tthhe mmillkk in a veeryy cclleean saauuceeaann annd\n",
      "guess: mode pt the milk in a very clean saucean and\n",
      "truth: mode put the milk in a very clean saucepan and\n",
      "\n",
      "\n",
      "\n",
      "input: wannderin ssinneer oo mmyy honneyy sweeet siineerr\n",
      "guess: wandering signer go my honey swet siner\n",
      "truth: wandering singer o my honey sweet singer\n",
      "\n",
      "\n",
      "\n",
      "input: wwoorrd baammbbeedaayy lliitteerallly mmysteeriouus ffeeeellin whichh hhass beeen vaarrioousslyy trransslaateed\n",
      "guess: word bambeday literaly mysterious feling which has ben variously translated\n",
      "truth: word bambeday literally mysterious feeling which has been variously translated\n",
      "\n",
      "train loss 0.80 eval_loss 0.87\n",
      "Epoch No:2\n",
      "Batch_num : 0\n",
      "Batch_num : 100\n",
      "Batch_num : 200\n",
      "Batch_num : 300\n",
      "Batch_num : 400\n",
      "Batch_num : 500\n",
      "Batch_num : 600\n",
      "Batch_num : 700\n",
      "Batch_num : 800\n",
      "Batch_num : 900\n",
      "Batch_num : 1000\n",
      "Batch_num : 1100\n",
      "Batch_num : 1200\n",
      "Batch_num : 1300\n",
      "Batch_num : 1400\n",
      "Batch_num : 1500\n",
      "Batch_num : 1600\n",
      "Batch_num : 1700\n",
      "Batch_num : 1800\n",
      "Batch_num : 1900\n",
      "Batch_num : 2000\n",
      "Batch_num : 2100\n",
      "Batch_num : 2200\n",
      "Batch_num : 2300\n",
      "Batch_num : 2400\n",
      "Batch_num : 2500\n",
      "Batch_num : 2600\n",
      "Batch_num : 2700\n",
      "Batch_num : 2800\n",
      "Batch_num : 2900\n",
      "Batch_num : 3000\n",
      "Batch_num : 3100\n",
      "Batch_num : 3200\n",
      "Batch_num : 3300\n",
      "Batch_num : 3400\n",
      "Batch_num : 3500\n",
      "Batch_num : 3600\n",
      "Batch_num : 3700\n",
      "Batch_num : 3800\n",
      "Batch_num : 3900\n",
      "Batch_num : 4000\n",
      "Batch_num : 4100\n",
      "Batch_num : 4200\n",
      "Batch_num : 4300\n",
      "Batch_num : 4400\n",
      "Batch_num : 4500\n",
      "Batch_num : 4600\n",
      "Batch_num : 4700\n",
      "Batch_num : 4800\n",
      "Batch_num : 4900\n",
      "Batch_num : 5000\n",
      "Batch_num : 5100\n",
      "Batch_num : 5200\n",
      "Batch_num : 5300\n",
      "Batch_num : 5400\n",
      "Batch_num : 5500\n",
      "Batch_num : 5600\n",
      "Batch_num : 5700\n",
      "Batch_num : 5800\n",
      "Batch_num : 5900\n",
      "Batch_num : 6000\n",
      "Batch_num : 6100\n",
      "Batch_num : 6200\n",
      "Batch_num : 6300\n",
      "Batch_num : 6400\n",
      "Batch_num : 6500\n",
      "Batch_num : 6600\n",
      "Batch_num : 6700\n",
      "Batch_num : 6800\n",
      "Batch_num : 6900\n",
      "Batch_num : 7000\n",
      "Batch_num : 7100\n",
      "Batch_num : 7200\n",
      "Batch_num : 7300\n",
      "Batch_num : 7400\n",
      "Batch_num : 7500\n",
      "Batch_num : 7600\n",
      "Batch_num : 7700\n",
      "Batch_num : 7800\n",
      "Batch_num : 7900\n",
      "Batch_num : 8000\n",
      "Batch_num : 8100\n",
      "Batch_num : 8200\n",
      "Batch_num : 8300\n",
      "Batch_num : 8400\n",
      "Batch_num : 8500\n",
      "Batch_num : 8600\n",
      "Batch_num : 8700\n",
      "Batch_num : 8800\n",
      "Batch_num : 8900\n",
      "Batch_num : 9000\n",
      "Batch_num : 9100\n",
      "\n",
      "\n",
      "input: tthee oovverrnment maay goo too aannd you ttooo rrooaareed bbureesss\n",
      "guess: the goverent may go to and you to rored bures\n",
      "truth: the government may go to and you too roared burgess\n",
      "\n",
      "\n",
      "\n",
      "input: vveery wweelll saaiid miisssuus wwiilkkiins rresolluuttely tto hheerssellf eef ii\n",
      "guess: very wel said misus wilkins resolutely to herself ef i\n",
      "truth: very well said missus wilkins resolutely to herself ef i\n",
      "\n",
      "\n",
      "\n",
      "input: of wwhich aan auunt of hherrs wwass aabbess wwhheere sshee\n",
      "guess: of which an aunt of hers was abes where shep\n",
      "truth: of which an aunt of hers was abbess where she\n",
      "\n",
      "\n",
      "\n",
      "input: wiiffee aandd siisstterr byy the eunuucchhs anndd wwomenn of tthhee\n",
      "guess: wife and sister by the eunuchs and women of the\n",
      "truth: wife and sister by the eunuchs and women of the\n",
      "\n",
      "\n",
      "\n",
      "input: yyet as thee prooffouund ddooccttoor hhaadd beeenn tteerrrifiedd at hhis\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "guess: yet as the profound doctor had ben terified at his\n",
      "truth: yet as the profound doctor had been terrified at his\n",
      "\n",
      "\n",
      "\n",
      "input: maany amon thhe ennttile roosseellytess reffuusedd too bbellievee tthaatt aa\n",
      "guess: many amon the gentile roselytes refused to believe that a\n",
      "truth: many among the gentile proselytes refused to believe that a\n",
      "\n",
      "\n",
      "\n",
      "input: hhee hhadd reetuurneedd ttoo rissoonn tthis timee fforr havving ddoonee\n",
      "guess: he had retured to rison this time for having done\n",
      "truth: he had returned to prison this time for having done\n",
      "\n",
      "\n",
      "\n",
      "input: olldd granddamm wiitthh hheerr ccllosse caa diissttaaff aanndd spindlee rushheedd\n",
      "guess: od grandam with her close ca distaf and spindle rushed\n",
      "truth: old grandam with her close cap distaff and spindle rushed\n",
      "\n",
      "train loss 0.76 eval_loss 0.88\n",
      "Epoch No:3\n",
      "Batch_num : 0\n",
      "Batch_num : 100\n",
      "Batch_num : 200\n",
      "Batch_num : 300\n",
      "Batch_num : 400\n",
      "Batch_num : 500\n",
      "Batch_num : 600\n",
      "Batch_num : 700\n",
      "Batch_num : 800\n",
      "Batch_num : 900\n",
      "Batch_num : 1000\n",
      "Batch_num : 1100\n",
      "Batch_num : 1200\n",
      "Batch_num : 1300\n",
      "Batch_num : 1400\n",
      "Batch_num : 1500\n",
      "Batch_num : 1600\n",
      "Batch_num : 1700\n",
      "Batch_num : 1800\n",
      "Batch_num : 1900\n",
      "Batch_num : 2000\n",
      "Batch_num : 2100\n",
      "Batch_num : 2200\n",
      "Batch_num : 2300\n",
      "Batch_num : 2400\n",
      "Batch_num : 2500\n",
      "Batch_num : 2600\n",
      "Batch_num : 2700\n",
      "Batch_num : 2800\n",
      "Batch_num : 2900\n",
      "Batch_num : 3000\n",
      "Batch_num : 3100\n",
      "Batch_num : 3200\n",
      "Batch_num : 3300\n",
      "Batch_num : 3400\n",
      "Batch_num : 3500\n",
      "Batch_num : 3600\n",
      "Batch_num : 3700\n",
      "Batch_num : 3800\n",
      "Batch_num : 3900\n",
      "Batch_num : 4000\n",
      "Batch_num : 4100\n",
      "Batch_num : 4200\n",
      "Batch_num : 4300\n",
      "Batch_num : 4400\n",
      "Batch_num : 4500\n",
      "Batch_num : 4600\n",
      "Batch_num : 4700\n",
      "Batch_num : 4800\n",
      "Batch_num : 4900\n",
      "Batch_num : 5000\n",
      "Batch_num : 5100\n",
      "Batch_num : 5200\n",
      "Batch_num : 5300\n",
      "Batch_num : 5400\n",
      "Batch_num : 5500\n",
      "Batch_num : 5600\n",
      "Batch_num : 5700\n",
      "Batch_num : 5800\n",
      "Batch_num : 5900\n",
      "Batch_num : 6000\n",
      "Batch_num : 6100\n",
      "Batch_num : 6200\n",
      "Batch_num : 6300\n",
      "Batch_num : 6400\n",
      "Batch_num : 6500\n",
      "Batch_num : 6600\n",
      "Batch_num : 6700\n",
      "Batch_num : 6800\n",
      "Batch_num : 6900\n",
      "Batch_num : 7000\n",
      "Batch_num : 7100\n",
      "Batch_num : 7200\n",
      "Batch_num : 7300\n",
      "Batch_num : 7400\n",
      "Batch_num : 7500\n",
      "Batch_num : 7600\n",
      "Batch_num : 7700\n",
      "Batch_num : 7800\n",
      "Batch_num : 7900\n",
      "Batch_num : 8000\n",
      "Batch_num : 8100\n",
      "Batch_num : 8200\n",
      "Batch_num : 8300\n",
      "Batch_num : 8400\n",
      "Batch_num : 8500\n",
      "Batch_num : 8600\n",
      "Batch_num : 8700\n",
      "Batch_num : 8800\n",
      "Batch_num : 8900\n",
      "Batch_num : 9000\n",
      "Batch_num : 9100\n",
      "\n",
      "\n",
      "input: bbe haansel fatt oor leann tthiiss morninn ii wiillll kiill\n",
      "guess: be hansel fat or len this morin i wil kil\n",
      "truth: be hansel fat or lean this morning i will kill\n",
      "\n",
      "\n",
      "\n",
      "input: tthhee banks are thhe finestt ii havve eevver sseeen iin\n",
      "guess: the banks are the finest i have ever sen in\n",
      "truth: the banks are the finest i have ever seen in\n",
      "\n",
      "\n",
      "\n",
      "input: ii oonnllyy wiisshh tto bbe allonnee yyoouu wwiilll eexxccuse mme\n",
      "guess: i only wish to be alone you wil excuse me\n",
      "truth: i only wish to be alone you will excuse me\n",
      "\n",
      "\n",
      "\n",
      "input: samee ttiimee too rrenderr iitt thee mmorre difffiicuult off aattaainmmeennt\n",
      "guess: same time to render it the more dificult of ataingment\n",
      "truth: same time to render it the more difficult of attainment\n",
      "\n",
      "\n",
      "\n",
      "input: inn a ffew yeearrss ttherree waass not an aaccrree bettwweeenn\n",
      "guess: in a few years there was not an acre betwwen\n",
      "truth: in a few years there was not an acre between\n",
      "\n",
      "\n",
      "\n",
      "input: aanndd wwrotee him\n",
      "guess: and wrote him\n",
      "truth: and wrote him\n",
      "\n",
      "\n",
      "\n",
      "input: stickk tthhaat hheldd uup tthee wwhiittee lliillyy loooseed aalll tthe\n",
      "guess: stick that held u the white lily losed al the\n",
      "truth: stick that held up the white lily loosed all the\n",
      "\n",
      "\n",
      "\n",
      "input: nut llyyinng ssnugly aas dooess aannyy cchheestnutt iinn iitts burr\n",
      "guess: nut lyin snugly as does any chestnut in its bur\n",
      "truth: nut lying snugly as does any chestnut in its bur\n",
      "\n",
      "train loss 0.71 eval_loss 0.73\n",
      "Epoch No:4\n",
      "Batch_num : 0\n",
      "Batch_num : 100\n",
      "Batch_num : 200\n",
      "Batch_num : 300\n",
      "Batch_num : 400\n",
      "Batch_num : 500\n",
      "Batch_num : 600\n",
      "Batch_num : 700\n",
      "Batch_num : 800\n",
      "Batch_num : 900\n",
      "Batch_num : 1000\n",
      "Batch_num : 1100\n",
      "Batch_num : 1200\n",
      "Batch_num : 1300\n",
      "Batch_num : 1400\n",
      "Batch_num : 1500\n",
      "Batch_num : 1600\n",
      "Batch_num : 1700\n",
      "Batch_num : 1800\n",
      "Batch_num : 1900\n",
      "Batch_num : 2000\n",
      "Batch_num : 2100\n",
      "Batch_num : 2200\n",
      "Batch_num : 2300\n",
      "Batch_num : 2400\n",
      "Batch_num : 2500\n",
      "Batch_num : 2600\n",
      "Batch_num : 2700\n",
      "Batch_num : 2800\n",
      "Batch_num : 2900\n",
      "Batch_num : 3000\n",
      "Batch_num : 3100\n",
      "Batch_num : 3200\n",
      "Batch_num : 3300\n",
      "Batch_num : 3400\n",
      "Batch_num : 3500\n",
      "Batch_num : 3600\n",
      "Batch_num : 3700\n",
      "Batch_num : 3800\n",
      "Batch_num : 3900\n",
      "Batch_num : 4000\n",
      "Batch_num : 4100\n",
      "Batch_num : 4200\n",
      "Batch_num : 4300\n",
      "Batch_num : 4400\n",
      "Batch_num : 4500\n",
      "Batch_num : 4600\n",
      "Batch_num : 4700\n",
      "Batch_num : 4800\n",
      "Batch_num : 4900\n",
      "Batch_num : 5000\n",
      "Batch_num : 5100\n",
      "Batch_num : 5200\n",
      "Batch_num : 5300\n",
      "Batch_num : 5400\n",
      "Batch_num : 5500\n",
      "Batch_num : 5600\n",
      "Batch_num : 5700\n",
      "Batch_num : 5800\n",
      "Batch_num : 5900\n",
      "Batch_num : 6000\n",
      "Batch_num : 6100\n",
      "Batch_num : 6200\n",
      "Batch_num : 6300\n",
      "Batch_num : 6400\n",
      "Batch_num : 6500\n",
      "Batch_num : 6600\n",
      "Batch_num : 6700\n",
      "Batch_num : 6800\n",
      "Batch_num : 6900\n",
      "Batch_num : 7000\n",
      "Batch_num : 7100\n",
      "Batch_num : 7200\n",
      "Batch_num : 7300\n",
      "Batch_num : 7400\n",
      "Batch_num : 7500\n",
      "Batch_num : 7600\n",
      "Batch_num : 7700\n",
      "Batch_num : 7800\n",
      "Batch_num : 7900\n",
      "Batch_num : 8000\n",
      "Batch_num : 8100\n",
      "Batch_num : 8200\n",
      "Batch_num : 8300\n",
      "Batch_num : 8400\n",
      "Batch_num : 8500\n",
      "Batch_num : 8600\n",
      "Batch_num : 8700\n",
      "Batch_num : 8800\n",
      "Batch_num : 8900\n",
      "Batch_num : 9000\n",
      "Batch_num : 9100\n",
      "\n",
      "\n",
      "input: uunnlessss iitt'ss aa ccanncerr in the stomaach ii ddonn'tt kknnoow\n",
      "guess: gunles it's a caner in the stomach i don't know\n",
      "truth: unless it's a cancer in the stomach i don't know\n",
      "\n",
      "\n",
      "\n",
      "input: the chhild ddemandds iit dooess nnorr iittss ownn tteerrroorr uundeerrsttands\n",
      "guess: the child demans it does nor its ow teror understands\n",
      "truth: the child demands it does nor its own terror understands\n",
      "\n",
      "\n",
      "\n",
      "input: aas soon aas eevveer oof my seeccoonnd ae i wass\n",
      "guess: as son as ever of my second age i was\n",
      "truth: as soon as ever of my second age i was\n",
      "\n",
      "\n",
      "\n",
      "input: the hhouuse of tthhe eeleephant\n",
      "guess: the house of the elephant\n",
      "truth: the house of the elephant\n",
      "\n",
      "\n",
      "\n",
      "input: acrroosss tthe cchhannneell\n",
      "guess: acros the chanel\n",
      "truth: across the channel\n",
      "\n",
      "\n",
      "\n",
      "input: thhe sstiirruu hhee tthhrreeww hhiss boody sshhaarrplly too tthhe rriightt\n",
      "guess: the stiru he threw his body sharly to the right\n",
      "truth: the stirrup he threw his body sharply to the right\n",
      "\n",
      "\n",
      "\n",
      "input: he ennterreed tthe guarrd rooom thheerree tthee guuaardss stoood ddrawnn\n",
      "guess: he enered the gurd rom there the gugars stod draw\n",
      "truth: he entered the guard room there the guards stood drawn\n",
      "\n",
      "\n",
      "\n",
      "input: laannd andd allll the piinnkkieess in aaess ttoo come willll\n",
      "guess: land and al the pinkies in ages to come wil\n",
      "truth: land and all the pinkies in ages to come will\n",
      "\n",
      "train loss 0.64 eval_loss 0.64\n",
      "Epoch No:5\n",
      "Batch_num : 0\n",
      "Batch_num : 100\n",
      "Batch_num : 200\n",
      "Batch_num : 300\n",
      "Batch_num : 400\n",
      "Batch_num : 500\n",
      "Batch_num : 600\n",
      "Batch_num : 700\n",
      "Batch_num : 800\n",
      "Batch_num : 900\n",
      "Batch_num : 1000\n",
      "Batch_num : 1100\n",
      "Batch_num : 1200\n",
      "Batch_num : 1300\n",
      "Batch_num : 1400\n",
      "Batch_num : 1500\n",
      "Batch_num : 1600\n",
      "Batch_num : 1700\n",
      "Batch_num : 1800\n",
      "Batch_num : 1900\n",
      "Batch_num : 2000\n",
      "Batch_num : 2100\n",
      "Batch_num : 2200\n",
      "Batch_num : 2300\n",
      "Batch_num : 2400\n",
      "Batch_num : 2500\n",
      "Batch_num : 2600\n",
      "Batch_num : 2700\n",
      "Batch_num : 2800\n",
      "Batch_num : 2900\n",
      "Batch_num : 3000\n",
      "Batch_num : 3100\n",
      "Batch_num : 3200\n",
      "Batch_num : 3300\n",
      "Batch_num : 3400\n",
      "Batch_num : 3500\n",
      "Batch_num : 3600\n",
      "Batch_num : 3700\n",
      "Batch_num : 3800\n",
      "Batch_num : 3900\n",
      "Batch_num : 4000\n",
      "Batch_num : 4100\n",
      "Batch_num : 4200\n",
      "Batch_num : 4300\n",
      "Batch_num : 4400\n",
      "Batch_num : 4500\n",
      "Batch_num : 4600\n",
      "Batch_num : 4700\n",
      "Batch_num : 4800\n",
      "Batch_num : 4900\n",
      "Batch_num : 5000\n",
      "Batch_num : 5100\n",
      "Batch_num : 5200\n",
      "Batch_num : 5300\n",
      "Batch_num : 5400\n",
      "Batch_num : 5500\n",
      "Batch_num : 5600\n",
      "Batch_num : 5700\n",
      "Batch_num : 5800\n",
      "Batch_num : 5900\n",
      "Batch_num : 6000\n",
      "Batch_num : 6100\n",
      "Batch_num : 6200\n",
      "Batch_num : 6300\n",
      "Batch_num : 6400\n",
      "Batch_num : 6500\n",
      "Batch_num : 6600\n",
      "Batch_num : 6700\n",
      "Batch_num : 6800\n",
      "Batch_num : 6900\n",
      "Batch_num : 7000\n",
      "Batch_num : 7100\n",
      "Batch_num : 7200\n"
     ]
    }
   ],
   "source": [
    "num_chars = len(char_labels)\n",
    "model = TransducerASR(num_inputs=num_chars+1, num_outputs=num_chars+1)\n",
    "trainer = Trainer(model=model, lr=0.0001)\n",
    "load_file = './model_epoch_2.pt'\n",
    "model.load_state_dict(torch.load(load_file))\n",
    "train = True\n",
    "save_file = './best_model.pt'\n",
    "if train:\n",
    "    t_loss, v_loss = trainer.fit(train_loader, eval_loader, num_epochs=10)\n",
    "    torch.save(model.state_dict(), save_file)\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "num_chars = len(char_labels)\n",
    "load_file = './asr_model.pt'\n",
    "model = TransducerASR(num_inputs=num_chars+1, num_outputs=num_chars+1)\n",
    "#model.load_state_dict(torch.load(load_file))\n",
    "\n",
    "#Compute WER with Greedy Search\n",
    "tester = Tester(model=model)\n",
    "y_test, y_pred = tester.test(test_loader)\n",
    "error = wer(y_test, y_pred)\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19916206205598397\n"
     ]
    }
   ],
   "source": [
    "#Compute WER with Beam Search\n",
    "tester = Tester(model=model, search=\"beam\")\n",
    "y_test, y_pred_beam = tester.test(test_loader)\n",
    "error_beam = wer(y_test, y_pred_beam)\n",
    "print(error_beam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19860849486540683\n"
     ]
    }
   ],
   "source": [
    "print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "transducer-example.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

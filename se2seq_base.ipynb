{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BbwgR5UdNkkm"
   },
   "source": [
    "# Base Model Seq2Seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q-iHU02C7fAj",
    "outputId": "118cc9b3-228a-4aee-cd32-3692a636d570"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/home/user1/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "Requirement already satisfied: unidecode in /home/user1/anaconda3/lib/python3.7/site-packages (1.2.0)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import string\n",
    "!pip install unidecode\n",
    "import unidecode\n",
    "import numpy as np\n",
    "import itertools\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CTfRgwxmjv1B"
   },
   "source": [
    "# Building blocks\n",
    "\n",
    "First, we will define the encode and decoder\n",
    "\n",
    "<img src=\"\" width=\"25%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "B7mLFyUG7kJH"
   },
   "outputs": [],
   "source": [
    "NULL_INDEX = 0\n",
    "\n",
    "encoder_dim = 1024\n",
    "predictor_dim = 1024\n",
    "joiner_dim = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "KE7j2T5EY33-"
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        \n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        \n",
    "        #src = [src len, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        \n",
    "        #embedded = [src len, batch size, emb dim]\n",
    "        \n",
    "        outputs, (hidden, cell) = self.rnn(embedded)\n",
    "        \n",
    "        #outputs = [src len, batch size, hid dim * n directions]\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        #cell = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        #outputs are always from the top hidden layer\n",
    "        \n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "Vlzca1orZDLa"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.output_dim = output_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        \n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n",
    "        \n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden, cell):\n",
    "        \n",
    "        #input = [batch size]\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        #cell = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        #n directions in the decoder will both always be 1, therefore:\n",
    "        #hidden = [n layers, batch size, hid dim]\n",
    "        #context = [n layers, batch size, hid dim]\n",
    "        \n",
    "        input = input.unsqueeze(0)\n",
    "        \n",
    "        #input = [1, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        \n",
    "        #embedded = [1, batch size, emb dim]\n",
    "                \n",
    "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
    "        \n",
    "        #output = [seq len, batch size, hid dim * n directions]\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        #cell = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        #seq len and n directions will always be 1 in the decoder, therefore:\n",
    "        #output = [1, batch size, hid dim]\n",
    "        #hidden = [n layers, batch size, hid dim]\n",
    "        #cell = [n layers, batch size, hid dim]\n",
    "        \n",
    "        prediction = self.fc_out(output.squeeze(0))\n",
    "        \n",
    "        #prediction = [batch size, output dim]\n",
    "        \n",
    "        return prediction, hidden, cell\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "sYSagKi-gHM4"
   },
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "        assert encoder.hid_dim == decoder.hid_dim, \\\n",
    "            \"Hidden dimensions of encoder and decoder must be equal!\"\n",
    "        assert encoder.n_layers == decoder.n_layers, \\\n",
    "            \"Encoder and decoder must have equal number of layers!\"\n",
    "        \n",
    "    def forward(self, src, trg, teacher_forcing_ratio = 0.90):\n",
    "        \n",
    "        #src = [src len, batch size]\n",
    "        #trg = [trg len, batch size]\n",
    "        #teacher_forcing_ratio is probability to use teacher forcing\n",
    "        #e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time\n",
    "        \n",
    "        batch_size = trg.shape[1]\n",
    "        trg_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        \n",
    "        #tensor to store decoder outputs\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        \n",
    "        #last hidden state of the encoder is used as the initial hidden state of the decoder\n",
    "        hidden, cell = self.encoder(src)\n",
    "        \n",
    "        #first input to the decoder is the <sos>\n",
    "        input = trg[0,:]\n",
    "        \n",
    "        for t in range(1, trg_len):\n",
    "            \n",
    "            #insert input token embedding, previous hidden and previous cell states\n",
    "            #receive output tensor (predictions) and new hidden and cell states\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "            \n",
    "            #place predictions in a tensor holding predictions for each token\n",
    "            outputs[t] = output\n",
    "            \n",
    "            #decide if we are going to use teacher forcing or not\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            \n",
    "            #get the highest predicted token from our predictions\n",
    "            top1 = output.argmax(1) \n",
    "            \n",
    "            #if teacher forcing, use actual next token as next input\n",
    "            #if not, use predicted token\n",
    "            input = trg[t] if teacher_force else top1\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "RWtkoXH6U8Pm"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "INPUT_DIM = 32\n",
    "OUTPUT_DIM = 32\n",
    "ENC_EMB_DIM = 32\n",
    "DEC_EMB_DIM = 32\n",
    "HID_DIM = 1024\n",
    "N_LAYERS = 2\n",
    "ENC_DROPOUT = 0.2\n",
    "DEC_DROPOUT = 0.2\n",
    "START_INDEX = 0\n",
    "BLANK_INDEX = 27\n",
    "PAD_INDEX = 30\n",
    "END_INDEX = 31\n",
    "\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
    "\n",
    "model = Seq2Seq(enc, dec, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "V0xeyb7Jf18_"
   },
   "outputs": [],
   "source": [
    "def greedy_search(self, x, T):\n",
    "  y_batch = []\n",
    "  B = len(x)\n",
    "  print(B, len(T))  \n",
    "\n",
    "  encoder_out = self.encoder.forward(x)\n",
    "  U_max = 200\n",
    "  for b in range(B):\n",
    "    t = 0; u = 0; y = [self.predictor.start_symbol]; predictor_state = self.predictor.initial_state.unsqueeze(0)\n",
    "    while t < T[b] and u < U_max:\n",
    "      predictor_input = torch.tensor([ y[-1] ]).to(x.device)\n",
    "      g_u, predictor_state = self.predictor.forward_one_step(predictor_input, predictor_state)\n",
    "      f_t = encoder_out[b, t]\n",
    "      h_t_u = self.joiner.forward(f_t, g_u)\n",
    "      argmax = h_t_u.max(-1)[1].item()\n",
    "      if argmax == NULL_INDEX:\n",
    "        t += 1\n",
    "      else: # argmax == a label\n",
    "        u += 1\n",
    "        y.append(argmax)\n",
    "    y_batch.append(y[1:]) # remove start symbol\n",
    "  return y_batch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ff9raB0jVGzN"
   },
   "source": [
    "# Some utilities\n",
    "\n",
    "Here we will add a bit of boilerplate code for training and loading data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5b17OQm4WdVy",
    "outputId": "0a7d7b17-0b23-4ee8-e690-e70564917f04"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('anndd doouubbtted maannyy ddayss aanndd ass the daayys iincreasseed iinncrreassed',\n",
       " 'and doubted many days and as the days increased increased')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "char_labels = ['<start>', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's',\n",
    "              't', 'u', 'v', 'w', 'x', 'y', 'z', '<blank>', ' ', \"'\", '<pad>','<end>']\n",
    "char_dict = {'<start>':0, 'a':1, 'b':2, 'c':3, 'd':4, 'e':5, 'f':6, 'g':7, 'h':8, 'i':9, 'j':10, 'k':11, 'l':12, 'm':13, \n",
    "             'n':14, 'o':15, 'p':16, 'q':17, 'r':18, 's':19, 't':20, 'u':21, 'v':22, 'w':23, 'x':24, 'y':25, 'z':26, \n",
    "             '<blank>':27, ' ':28, \"'\":29, '<pad>':30,'<end>':31}\n",
    "char_inv_dict = {0:'', 1:'a', 2:'b', 3:'c', 4:'d', 5:'e', 6:'f', 7:'g', 8:'h', 9:'i', 10:'j', 11:'k', 12:'l',\n",
    "                 13:'m', 14:'n', 15:'o', 16:'p', 17:'q', 18:'r', 19:'s', 20:'t', 21:'u', 22:'v', 23:'w', 24:'x', 25:'y',\n",
    "                 26:'z', 27:'', 28:' ', 29:\"'\", 30:'', 31:''}\n",
    "\n",
    "class TextDataset(torch.utils.data.Dataset):\n",
    "  def __init__(self, lines, batch_size):\n",
    "    lines = list(filter((\"\\n\").__ne__, lines))\n",
    "\n",
    "    self.lines = lines # list of strings\n",
    "    collate = Collate()\n",
    "    self.loader = torch.utils.data.DataLoader(self, batch_size=batch_size, num_workers=0, shuffle=True, collate_fn=collate)\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.lines)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    line = self.lines[idx].replace(\"\\n\", \"\")\n",
    "    line =line.lower()\n",
    "    line = \"\".join(c for c in line if c in char_labels)\n",
    "    line = unidecode.unidecode(line) # remove special characters\n",
    "    x = \"\".join(c*random.randint(0,1) if c in (\"gjp\") else c*random.randint(1,2) for c in line) \n",
    "    x = ' '.join(x.split())\n",
    "    y = line\n",
    "    return (x,y)\n",
    "\n",
    "def encode_string(s):\n",
    "\n",
    "  return [char_dict[c] for c in s]\n",
    "\n",
    "def decode_labels(l):\n",
    "  return \"\".join([char_inv_dict[c] for c in l])\n",
    "\n",
    "class Collate:\n",
    "  def __call__(self, batch):\n",
    "    \"\"\"\n",
    "    batch: list of tuples (input string, output string)\n",
    "    Returns a minibatch of strings, encoded as labels and padded to have the same length.\n",
    "    \"\"\"\n",
    "    x = []; y = []\n",
    "    batch_size = len(batch)\n",
    "    for index in range(batch_size):\n",
    "      x_,y_ = batch[index]\n",
    "      x.append([0]+encode_string(x_))\n",
    "      y.append([0]+encode_string(y_))\n",
    "\n",
    "    # pad all sequences to have same length\n",
    "    T = [len(x_) for x_ in x]\n",
    "    U = [len(y_) for y_ in y]\n",
    " \n",
    "    T_max = max(T)\n",
    "    U_max = max(U)\n",
    "    for index in range(batch_size):\n",
    "      x[index] += [PAD_INDEX] * (T_max - len(x[index])) +[31]\n",
    "      x[index] = torch.tensor(x[index])\n",
    "      y[index] += [PAD_INDEX] * (U_max - len(y[index]))+[31]\n",
    "      y[index] = torch.tensor(y[index])\n",
    "\n",
    "    # stack into single tensor\n",
    "    x = torch.stack(x)\n",
    "    y = torch.stack(y)\n",
    "    #T = torch.tensor(T)\n",
    "    #U = torch.tensor(U)\n",
    "\n",
    "    return (x,y) #,T,U)\n",
    "\n",
    "def gen_features(filename, max_len):\n",
    "    with open(filename, \"r\") as f:\n",
    "        t_lines = f.readlines()\n",
    "        t_lines = t_lines[1:]\n",
    "        t_lines = [line.split('\\t')[2] for line in t_lines ]\n",
    "        lines = []\n",
    "\n",
    "        for line in t_lines:\n",
    "            temp = line.split(' ')\n",
    "            count = len(temp)//max_len\n",
    "            resid = len(temp)%max_len\n",
    "    \n",
    "            k = 0\n",
    "            while(count > 0):    \n",
    "                short_line = ' '.join(temp[k:k+max_len]) \n",
    "                lines.append(short_line)\n",
    "                count = count - 1\n",
    "                k = k + max_len\n",
    "            if(resid > 0):\n",
    "                lines.append(' '.join(temp[k:k+resid]))   \n",
    "    return(lines)            \n",
    "\n",
    "\n",
    "train_filename = \"../LibriSpeech/train-clean-100/transcripts_460.tsv\"\n",
    "eval_filename = \"../LibriSpeech/dev-clean/transcripts.tsv\"\n",
    "test_filename = \"../LibriSpeech/test-clean/transcripts.tsv\"\n",
    "\n",
    "\n",
    "train_lines = gen_features(train_filename, 10)\n",
    "eval_lines = gen_features(eval_filename, 10)\n",
    "test_lines = gen_features(test_filename, 10)\n",
    "train_set = TextDataset(train_lines, batch_size=32)\n",
    "eval_set = TextDataset(test_lines, batch_size=8)\n",
    "test_set = TextDataset(test_lines, batch_size=8)\n",
    "train_set.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(32, 32)\n",
       "    (rnn): LSTM(32, 1024, num_layers=2, dropout=0.2)\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(32, 32)\n",
       "    (rnn): LSTM(32, 1024, num_layers=2, dropout=0.2)\n",
       "    (fc_out): Linear(in_features=1024, out_features=32, bias=True)\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "        \n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 25,495,584 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = PAD_INDEX)\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=0.98)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, batch in enumerate(iterator):\n",
    "        \n",
    "        src, trg = batch\n",
    "        trg = trg.transpose(1,0)\n",
    "        src = src.transpose(1,0)\n",
    "        optimizer.zero_grad()\n",
    "        src = src.to(device)\n",
    "        trg = trg.to(device)\n",
    "        output = model(src, trg)\n",
    "        \n",
    "        #trg = [trg len, batch size]\n",
    "        #output = [trg len, batch size, output dim]\n",
    "        \n",
    "        output_dim = output.shape[-1]\n",
    "        \n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        trg = trg[1:].reshape(-1)\n",
    "        \n",
    "        #trg = [(trg len - 1) * batch size]\n",
    "        #output = [(trg len - 1) * batch size, output dim]\n",
    "        \n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, batch in enumerate(iterator):\n",
    "\n",
    "            src, trg = batch\n",
    "            trg = trg.transpose(1,0)\n",
    "            src = src.transpose(1,0)\n",
    "            src = src.to(device)\n",
    "            trg = trg.to(device)\n",
    "            output = model(src, trg, 0) #turn off teacher forcing\n",
    "\n",
    "            #trg = [trg len, batch size]\n",
    "            #output = [trg len, batch size, output dim]\n",
    "\n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            trg = trg[1:].reshape(-1)\n",
    "\n",
    "            #trg = [(trg len - 1) * batch size]\n",
    "            #output = [(trg len - 1) * batch size, output dim]\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 20\n",
    "CLIP = 1\n",
    "best_valid_loss = float('inf')\n",
    "train_loss = []\n",
    "valid_loss = []\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    tr_loss = train(model, train_set.loader, optimizer, criterion, CLIP)\n",
    "    train_loss.append(tr_loss)\n",
    "    v_loss = evaluate(model, eval_set.loader, criterion)\n",
    "    valid_loss.append(v_loss)\n",
    "    lr_scheduler.step()\n",
    "    print(tr_loss, v_loss)\n",
    "    if(best_valid_loss > v_loss):\n",
    "        best_valid_loss = v_loss\n",
    "        save_file = './base_model.pt'\n",
    "        torch.save(model.state_dict(), save_file)\n",
    "print(train_loss, valid_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, iterator):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    y_pred = []\n",
    "    y_test = []\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for i, batch in enumerate(iterator):\n",
    "\n",
    "            src, trg = batch\n",
    "            trg = trg.transpose(1,0)\n",
    "            src = src.transpose(1,0)\n",
    "            src = src.to(device)\n",
    "            trg = trg.to(device)\n",
    "            output = model(src, trg, 0) #turn off teacher forcing\n",
    "\n",
    "            output = output.argmax(-1)\n",
    "            output = output.transpose(1,0)\n",
    "            trg = trg.transpose(1,0)\n",
    "            y_p = output.cpu().detach().numpy()\n",
    "            y_t = trg.cpu().detach().numpy()\n",
    "            y_p_s = []\n",
    "            y_t_s = []\n",
    "            for j in range(y_p.shape[0]):\n",
    "                y_p_s.append(decode_labels(y_p[j,:]))\n",
    "                y_t_s.append(decode_labels(y_t[j,:]))\n",
    "            y_pred.append(y_p_s)\n",
    "            y_test.append(y_t_s)\n",
    "            if i == 2:\n",
    "                break\n",
    "    return y_pred, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred, y_test = inference(model, test_set.loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['through the street on a shrove tuesday a week afterwards', 'not give a halfpenny to know said my uncle tobyed', 'by grain they separated the pile sorting each kind too', 'if the country were opposed to to bribery it is', 'darkness of failure that is evolving into the sunlight of', 'opening linena e', \"or when he che'd't it or when he roasted it byoky\", 'the peers answered this declaration by a protests'], ['without thee o sovereign goodness i could not know how', 'the mud at the bottom of the pond than theyoffey', 'other advertisers who would note his independence and', 'lying beneath the wheels of the cannons', 'our virtues although naturally they are not these snicer and', 'and again lifted sufficiently to enable us to see anything', 'beholding a possible life which she had sinned herself away', 'he stepped back and oliver came forward it was too'], ['and milan was farther from rock island than either carbon', 'you masked her aoe', 'between the high windowl bubbles of the shoe shops and', 'it was that same day that the three boys from by', 'must ride in comfort others spread their clothes out on', 'and a vigorous offensive war be waged against these mighty', 'being the property of spanishsand', 'from advertisement before anything else he was indeed compelled to']]\n"
     ]
    }
   ],
   "source": [
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['through the street on a shrove tuesday a week afterwards', 'not give a halfpenny to know said my uncle toby', 'by grain they separated the pile sorting each kind to', 'if the country were opposed to to bribery it is', 'darkness of failure that is evolving into the sunlight of', 'opening line', \"or when he chew'd it or when he roasted it\", 'the peers answered this declaration by a protest'], ['without thee o sovereign goodness i could not know how', 'the mud at the bottom of the pond than they', 'other advertisers who would note his independence', 'lying beneath the wheels of the cannons', 'our virtues although naturally they are not those sincere and', 'and again lifted sufficiently to enable us to see anything', 'beholding a possible life which she had sinned herself away', 'he stepped back and oliver came forward it was too'], ['and milan was farther from rock island than either carbon', 'you masked her', 'between the high windowy bulks of the shoe shops and', 'it was that same day that the three boys from', 'must ride in comfort others spread their clothes out on', 'and a vigorous offensive war be waged against these mighty', 'being the property of spanish', 'from advertisement before anything else he was indeed compelled to']]\n"
     ]
    }
   ],
   "source": [
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "place in the morning of the french army when they were all right and a\n",
      "platon karataev of whom he had heard from pierre his love for natasha\n",
      "------------------\n",
      "who had been at the same time he was an air of a man who was already an\n",
      "whole face as she had so often seen it in the stillness of the night\n",
      "------------------\n",
      "have you heard of the same i shall be in a man who was a single and\n",
      "has lived through\n",
      "------------------\n",
      "french soldiers were silent and silently still his head and heard\n",
      "freedom\n",
      "------------------\n",
      "a commander of the rostovs were standing the french and the soldiers\n",
      "a happiness he had long forgotten and of which he had not even been\n",
      "------------------\n",
      "what a story of the emperor alexander in moscow and what is it asked th\n",
      "what pierre had told them princess mary did not express her opinion of\n",
      "------------------\n",
      "speaking of his face and the french were already been able to sing and\n",
      "special enclosure where there are oatsthat this very ram swelling\n",
      "------------------\n",
      "authority and the same in the middle of the russian army with a smile a\n",
      "autumn and which dessalles called les fils de la vierge in front was\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "t = trg.transpose(1,0)\n",
    "a = a1.cpu()\n",
    "b = t.cpu()\n",
    "b = b.detach().numpy()\n",
    "a = a.detach().numpy()\n",
    "for i in range(a.shape[0]):\n",
    "   print(decode_labels(a[i,:]))\n",
    "   print(decode_labels(b[i,:]))\n",
    "   print(\"------------------\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "transducer-example.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
